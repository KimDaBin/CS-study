# 알고리즘
1. 알고리즘 분석
	1. 시간 복잡도와 공간복잡도
	2. 알고리즘의 정당성 증명
2. [알고리즘 설계 패러다임](#알고리즘-설계-패러다임)
	1. 완전 탐색
	2. [분할 정복](#분할-정복)
	3. [동적 계획법](#동적-계획법)
	4. 탐욕법
	5. 조합 탐색
	6. [파라메트릭 서치](#파라메트릭-서치)
3. [유명한 알고리즘](#유명한-알고리즘)
	1. 수치해석
	2. [정수론](#정수론)
		1. [소수](#소수)
		2. [유클리드 알고리즘](#유클리드-알고리즘)
		3. [모듈라 연산](#모듈라-연산)
		4. [이항 계수](#이항-계수)
	3. 계산 기하
4. [기초 자료구조](#기초-자료구조)
	1. [비트마스크](#비트마스크)
	2. [부분 합](#부분-합)
	3. [선형 자료구조](#선형-자료구조)
	4. [큐와 스택, 데크](#큐와-스택과-데크)
	5. 문자열
5. [트리](#트리)
	1. [트리의 구현과 순회](#트리의-구현과-순회)
	2. [이진 탐색트리](#이진-탐색트리)
	3. [우선순위 큐와 힙](#우선순위-큐와-힙)
	4. 구간 트리
	5. 상호 배타적 집합
	6. [트라이](#트라이-Trie)
6. [그래프](#그래프)
   1. [그래프의 표현과 정의](#그래프의-표현과-정의)
   2. [DFS](#DFS)
   3. [BFS](#BFS)
   4. [최단 경로 알고리즘](#최단-경로-알고리즘)
      1. [다익스트라](#다익스트라)
      2. [벨만-포드](#벨만-포드)
      3. [플로이드의 모든 쌍 최단 거리 알고리즘](#플로이드의-모든-쌍-최단-거리-알고리즘)
   5. [최소 스패닝 트리](#최소-스패닝-트리)
      1. [크루스칼의 최소 스패닝 트리 알고리즘](#크루스칼의-최소-스패닝-트리-알고리즘)
      2. [프림의 최소 스패닝 트리 알고리즘](#프림의-최소-스패닝-트리-알고리즘)
   6. 네트워크 유량
      1. 포드-풀커슨 알고리즘
      2. 네트워크 모델링
      3. 이분 매칭
7. 정렬
	1. 삽입 정렬, 선택 정렬, 버블 정렬
	2. 병합 정렬
	3. 힙 정렬
	4. 퀵 정렬
	5. 기수 정렬
	6. 계수 정렬
	7. 셸 정렬

<br>

# 알고리즘 설계 패러다임
# 분할 정복
## 분할 정복(Divide and Conquer)이란?
한 문제를 둘 이상의 **부분 문제(sub-problem)** 로 나누어 해결하고 이를 합쳐 원래 문제를 해결하는 기법

분할 정복 알고리즘은 다음과 같이 세 부분으로 나누어서 생각해볼 수 있다.

1. **분할(Divide)** : 원래 문제를 분할하여 더 작은 하위 문제들 나눈다.

2. **정복(Conquer)** : 하위 문제 각각을 재귀적으로 해결

3. **병합(merge)** : 하위 문제들의 답을 합쳐서 원래 문제를 해결

<br>

분할 정복을 적용하기 위해서는 문제에 다음과 같은 몇 가지 특성이 성립해야 한다.

1. 부분 문제로 나누는 자연스러운 방법이 있어야 한다.

2. 부분 문제의 답을 조합해 원래 문제의 답을 계산하는 효율적인 방법이 있어야 한다.

   *(분할 정복을 사용한다고 무작정 효율이 좋아지는 것은 아니다.)*

<br>

## 분할 정복의 장/단점
- 장점 👍
  - 문제를 나눔으로써 어려운 문제를 해결할 수 있다는 장점이 있다. 그리고 이 방식이 그대로 사용되는 효율적인 알고리즘들도 여럿 있으며, 문제를 나누어 해결한다는 특징상 병렬적으로 문제를 해결하는 데 큰 강점이 있다.
  - 보통, 분할 정복의 경우 작은 문제로 분할함으로써 같은 작업을 더 빠르게 처리할 수 있게 해준다. (수행 시간 감소)

- 단점 👎
  - 함수를 재귀적으로 호출한다는 점에서 함수 호출로 인한 오버헤드가 발생하며, 스택에 다양한 데이터를 보관하고 있어야 하므로 스택 오버플로우가 발생하거나 과도한 메모리 사용을 하게 되는 단점이 있다.

<br>

## 일반적인 재귀 호출과 다른 점 

<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131237135-fd55bdac-c852-4681-b3f3-a6c4093fff28.png" width="50%">
</p>

* 분할 정복이 일반적인 재귀 호출과 다른 점은 **문제를 한 조각과 전체를 나누는 대신 거의 같은 크기의 부분 문제로 나누는 것** 이다.

* 보통 재귀 함수를 사용해서 분할 정복 알고리즘을 구현하지만, 분할 정복이라고 해서 반드시 재귀 함수를 이용하는 것은 아니다. 함수 호출시 발생하는 오버헤드를 없애기 위해서 스택이나 큐 등을 이용하는 경우도 있다.

<br>

## 분할 정복 알고리즘 활용 예시

분할 정복이 쓰이는 예는 **이분검색, 병합정렬, 퀵정렬, 최대값 찾기, 임계값의 결정, 쉬트라센 행렬곱셈 알고리즘** 등이 있다.

<br>

### 병합 정렬과 퀵정렬 (같은 문제를 어느 단계에서 해결하느냐에 따른 구분)

병합 정렬(merge sort)과 퀵 정렬(quick sort)은 분할 정복 패러다임을 기반으로 해서 만들어진 대표적인 정렬 알고리즘이다.

이 두 알고리즘은 같은 아이디어로 정렬을 수행하지만 시간이 많이 걸리는 작업을 **분할 단계**에서 하느냐, **병합 단계**에서 하느냐가 다르다.

이렇게 같은 문제를 해결하는 알고리즘이더라도 어떤 식으로(어느 단계에서) 분할하느냐에 따라 다른 알고리즘이 될 수 있다.

<br>

### 병합 정렬

<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131238392-d6591d56-4690-48b0-83c9-7e027e2fcda4.png" width="40%">
</p>

* 전체 수행 시간은 **병합 과정**에 의해 지배된다.

* **O(n)** 시간이 걸리는 과정을 **재귀 호출 후에 진행** (병합 과정)

   문제의 수는 항상 절반으로 나눠지기 때문에 필요한 단계 수는 **O(logn)**

* 시간 복잡도 : 항상 **O(nlogn)** 으로 일정

<br>

### 퀵 정렬
<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131238476-bd4f5db7-48f1-41f2-bca7-f888cb02f133.png" height="200"> <img src="https://user-images.githubusercontent.com/33649908/131238219-42fd1206-d152-4924-bfa2-0f53e122fe58.png" height="200">
</p>

* 전체 수행 시간은 두개 부분 문제로 나누는 **파티션(partition) 과정**에 의해 지배된다. 분할된 두 부분 문제가 비슷한 크기로 비슷한 크기로 나눠진다는 보장이 없기 때문에, 이를 비슷한 크기로 나누는 좋은 기준을 선택하는 것은 퀵정렬에서 중요한 요소이다.

* **O(n)** 시간이 걸리는 과정을 **재귀 호출 전**에 진행 (분할)

   문제의 수가 항상 절반으로 나누어 진다는 보장이 없기 때문에 필요한 단계수를 정확히 계산하기 힘들다.
   
   **최악의 경우 n, 평균적인 경우 logn**만큼의 단계가 필요하다.

* 시간 복잡도 : 최악 = **O(n^2)**, 평균 = **O(nlogn)**

<br>

### 관련 문제
[백준 1629번 곱셈](https://www.acmicpc.net/problem/1629)

[백준 10830번 행렬 제곱](https://www.acmicpc.net/problem/10830)

<br>

# 동적 계획법
## 동적 계획법(Dynamic Programming, DP)이란?

동적 계획법은 주어진 문제를 풀기 위해서, 문제를 여러 개의 **하위 문제(subproblem)** 로 나누어 푼 다음, 그것을 결합하여 해결하는 방식이다.

<br>

<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131238559-1a8c7588-3713-475d-b740-69629b9b9fb4.png" width="50%">
</p>

동적 계획법은 처음 주어진 문제를 더 작은 문제들로 나눈 뒤 각 조각의 답을 계산하고, 이 답들로부터 원래 문제에 대한 답을 계산해 낸다는 점에서 분할 정복(Divide and Conquer)과 비슷하다. 하지만 가장 큰 차이점은 **동적 계획법에서는 쪼개진 작은 문제가 중복되지만, 분할 정복은 절대로 중복될수가 없다는 점**이다.

다시 말하면, 동적 계획법과 분할 정복의 차이는 **문제를 나누는 방식**이다. 동적 계획법에서는 어떤 부분 문제는 두 개 이상의 문제를 푸는데 사용될 수 있기 때문에, 이 문제의 답을 여러 번 계산하는 대신 **한 번만 계산하고 그 결과를 재활용함으로써 속도를 향상**시킬 수 있다. 이때 이미 계산한 값을 저장해 두는 메모리를 캐시(cache)라고 부르며, 두 번 이상 계산되는 부분 문제를 중복되는 **부분 문제(overlapping subproblems)** 라고 부른다.

<br>

* **동적 계획법의 조건**

   두 가지 속성을 만족해야 동적 계획법으로 문제를 풀 수 있다.

1. **Overlapping Subproblem**
   : 중복되는 부분 문제(overlapping subproblem) 는 어떤 문제가 여러 개의 부분 문제(subproblem)으로 쪼개질 수 있을 때 사용하는 용어이다. 이때 '부분 문제'란, 항상 새로운 부분 문제를 생성해내기 보다는 계속해서 같은 부분 문제가 여러 번 재사용되거나 재귀 알고리즘을 통해 해결되는 문제를 가리킨다.

2. **Optimal Substructure**
   : 최적 부분구조(optimal substructure)는 어떤 문제의 최적의 해결책이 그 부분 문제의 최적의 해결책으로 부터 설계될 수 있는 경우를 말한다. 즉, 최적 부분구조 일때 문제의 정답을 작은 문제의 정답에서부터 구할 수 있다. 이 속성은 동적 계획법이나 그리디 알고리즘의 유용성을 판별하는데 사용되기도 한다.
   

<br>

* **메모리제이션(Memorization)**

메모이제이션은 컴퓨터 프로그램이 동일한 계산을 반복해야 할 때, **이전에 계산한 값을 메모리에 저장함**으로써 동일한 계산의 반복 수행을 제거하여 **프로그램 실행 속도를 빠르게 하는 기술**이다. 동적 계획법의 핵심이 되는 기술이다.

동적 계획법에서 각 문제는 한 번만 풀어야 한다. (중복되는 부분 문제를 여러번 풀지 않는다는 뜻) Optimal Substructure를 만족하기 때문에 같은 문제는 구할 때마다 정답이 같다. 따라서 정답을 한 번 구했으면 그 정답을 캐시에 메모해놓는다. 이렇게 메모하는 것을 코드의 구현에서는 배열에 저장하는 것으로 할 수 있다. 이를 메모리제이션이라고 한다.

<br>

## 동적 계획법의 장/단점

- 장점 👍
  - 필요한 모든 가능성을 고려해서 구현하므로 항상 최적의 결과를 얻을 수 있다.
  
  - 메모리에 저장된 값을 사용하므로 큰 문제를 빠른 속도로 해결하여 최적의 해를 찾아낼 수 있다.

- 단점 👎
  - 모든 가능성에 대한 고려가 불충분할 경우 최적의 결과를 보장할 수 없다.
  
  - 다른 방법론에 비해 많은 메모리 공간을 요구한다.
  

<br>

## 동적 계획법의 구현 방법

동적 계획법의 구현 방식에는 두 가지 방법이 있다.

1. **Top-down** : 큰 문제를 작은 문제로 쪼개면서 푼다. **재귀**로 구현
2. **Bottom-up** : 작은 문제부터 차례대로 푼다. **반복문**으로 구현

Top-down과 Botton-up의 시간복잡도 차이는 문제에 따라 다를 수 있으므로 정확히 알 수는 없다. Top-down은 재귀 호출을 하기때문에 스택의 사용으로 시간이 더 걸릴 것이라고 생각할 수 있겠지만, 실제로 그 차이는 크지 않다. 

(다만, 파이썬의 경우 재귀 호출 시 스택 오버 플로우(stack overflow)가 발생할 수 있기 때문에, Bottom-up으로 구현하는 것이 좋다. C++과 Java에서는 재귀로 구현하는 것이 크게 문제가 되지 않는다.)

💡 Top-down으로만 해결가능하거나 Bottom-up으로만 해결가능한 문제는 극히 드문 경우이므로, 아무거나 선택해서 사용하면 된다.

피보나치 수열을 예로 들면 다음과 같다.

* Top-down 방식
``` java
f (int n) {
  if n == 0 : return 0
  elif n == 1: return 1
  if dp[n] has value : return dp[n]
  else : dp[n] = f(n-2) + f(n-1)
         return dp[n]
}
```

* Bottom-up 방식
``` java
f (int n){
  f[0] = 0
  f[1] = 1
  for (i = 2; i <= n; i++) {
   f[i] = f[i-2] + f[i-1]
  }
  return f[n]
}
```
<br>

## 동적 계획법의 활용 예시

동적 계획법의 예시로는 **피보나치 수열 구하기, 이항계수 구하기, 최단경로의 플로이드 알고리즘, 최적화 문제, 외판원 문제** 등이 있다.

<br>

### 관련 문제
[백준 2294번 동전2](https://www.acmicpc.net/problem/2294)

[백준 1463번 1로 만들기](https://www.acmicpc.net/problem/1463)

<br>

----

# 파라메트릭 서치
결정 문제란 예 아니오 형태의 답만이 나오는 문제들을 가리킨다.

어떤 문제에서 최대가 되는 k를 구하려는 최적화 문제를 결정 문제로 바꾸는 것은 다음과 같다.

__1. k가 x이상일때 문제가 해결되는지?__

__2. 문제 해결이 안된다면 y값을 조절시켜 변경된 x이상일 때는 문제가 해결되는지?__

__3. x값 조절은 이분법을 사용__

위 과정을 반복하면 x는 문제가 해결되는 최적의 값으로 수렴하게 된다.

이렇게 결국 원래 문제인 최대값 k를 구할 수 있다. 같은 매커니즘으로 최솟값도 찾을 수 있는데, 이러한 알고리즘을 일명 파라메트릭 서치라고 부른다.

x값 조절을 이분법으로 하기 때문에, 바이너리 서치와 매우 유사하다.

하지만 차이점이 있는데, 바이너리 서치는 찾고자하는 특정한 값과 정렬된 검색 공간의 가운데에 있는 원소를 비교하여 해당 값을 찾으면 리턴, 못 찾으면 -1을 리턴해준다.

파라메트릭 서치는 특정한 값을 찾는것이 아니라 검색 공간의 가운데 값이 해당 값이 문제를 해결할 수 있는지 판단하고 범위를 좁히는데, 파라메트릭 서치는 바이너리 서치와 다르게 특정한 목표값이 없기 때문에 중간에 리턴을 할 수가 없고 반드시 값이 수렴하기 때문에 -1을 리턴하지 않는다는 차이가 있다. 

아래 문제는 파라메트릭 서치로 해결할 수 있는 가장 간단한 문제들 중 하나이다.

> __n개의 줄을 잘라서 길이(k)가 모두 동일한 m개의 줄로 만들 때, 잘라진 줄의 길이 k의 최대값을 구하라.__
> 
> __항상 n ≦ m 이며, n는 1이상 10,000이하의 정수이고, m은 1이상 1,000,000이하의 정수이다.__
> 
> __그리고 주어진 n개의 줄의 최대 길이는 2^31 - 1보다 작거나 같은 자연수이다.__
>
> __테스트 케이스로 n = 4, m = 11__
>
> __n개의 줄의 길이는 각각 802, 743, 457, 539가 주어졌다.__

최대의 k을 찾는 위 최적화 문제를 k가 x라면 줄을 m개 만들 수 있나? 라는 결정문제로 바꾸어서 풀 수 있다.

__1. k가 x라면 줄을 m개 만들 수 있나?__

__2. m개를 만들 수 없을때, x값을 줄임.__

__3. m개를 만들 수 있을때, x값을 늘림.__

__4. y+1이 m개를 만들 수 없으면 y가 k가 됨__

파라메트릭 서치 문제를 풀 때 초기 정의역x를 반드시 유효한 답을 도출할 수 있도록 잡아야 한다.

parametric_search(int min, int max) 에서  일반적으로 min은 0으로 잡으면 되고 max값을 넉넉히 잡으면 좋다.

위 문제 같은 경우는 만약 n = 4, m = 4이고, n개의 줄이 800, 1, 1, 1같은 테스트 케이스를 고려하면,

k = 200이 되므로 일반적으로 max범위는 n의 줄 길이 중 최대 길이인 800로 잡으면 된다.

```
// 수도코드
parametric_search(min, max){
	if(min > max) return max; // 기저조건, 값이 수렴

	x = (min + max)/2; // 범위 조절, 판단을 위한 x값을 정의
	
	// 있다 -> x값을 늘림 (최적값 수렴)
	// 없다 -> x값을 줄임
	if(isPossible(x)) return parametric_search(x+1, max);
	else return parametric_search(min, x-1);
}

isPossible(x){
	// 일반적으로 결정 판단을 위한 값을 구할때, 이 문제보다 훨씬 난이도가 높고 복잡하다.
	count = 0; // 결정 판단을 위한 값
	for(rope_length : rope_list) 
		count += rope_length / x; // x로 몇개의 줄을 만들 수 있는지
	
	// m개를 만들 수 있다, 없다
	if(count >= m) retrun true;
	else return false;
}
```

재귀의 기저조건인 if(min > max) return max;는 문제마다 차이가 있다.

어떤 문제는 if(min > max) return min; 일수도 있기 때문에 무조건 위 조건을 따라선 안되고 해당 문제를 이해하고 알맞은 조건을 기저조건으로 설정하여야 한다.

그리고 일반적으로 min은 left 또는 start, max는 right 또는 edn, x는 pivot 또는 mid 라는 변수명을 사용한다. 

지금 까지 본 문제는 파라메트릭 서치의 개념을 설명하기 위한 기초적인 문제였다면, 아래문제는 파라메트릭 서치의 응용 문제라고 할 수 있다. 

응용 문제같은 경우는 애초에 파라메트릭 서치 알고리즘을 이용해서 문제를 풀어야겠다는 아이디어 자체를 떠올리기가 쉽지않다...

[백준 1300번 : K번째 수](https://www.acmicpc.net/problem/1300)

[백준 12015번 : 가장 긴 증가하는 부분 수열 2](https://www.acmicpc.net/problem/12015)

# 유명한 알고리즘

# 정수론
정수론(Number Theory)은 각종 수의 성질을 대상으로 하며 기하학, 대수학, 해석학과 함께 수학의 주요한 분야들 중 하나이다. 

정수론의 현실 세계에서의 쓰임새는 다른 수학 분야에 비해 적지만, 컴퓨터가 발달되면서 사용빈도가 늘었다. 암호학의 기본 이론도 이 정수론을 기본으로 하고 있으며, 정보와 관련된 이론들도 상당 부분 정수론을 기본으로 한다. 

그 이유는 컴퓨터에서 정수는 정확한 값을 가질 수 있기 때문이다. 실수형 타입의 경우에는 round off error 때문에 오차가 생기고, 이 오차는 계산을 거듭할수록 걷잡을 수 없이 커지기 때문이다.
게다가 컴퓨터에서의 수 표현은 수를 표현할 저장공간의 한계상 정수론에서 말하는 시계 산술을 사용한다.

이름은 '정수론'이지만 기초 수준에서는 정수보다는 자연수, 그중에서도 소수를 중점적으로 다룬다. 자연수는 1과 소수, 그리고 합성수로 이루어져 있는데, 합성수들은 소수의 곱으로 생각할 수 있기 때문에 결국 소수가 다른 정수들보다 더 중요한 대우를 받게 된다.

음의 정수는 잘 다뤄지지 않는데, 대부분의 곱셈에 관련된 문제에서 양의 정수에 -1을 곱하는 것으로 음의 정수를 다룰 수 있기 때문이다. 

## 소수
소수를 한마디로 설명하면, 1보다 큰 자연수 중 1과 자기 자신만을 약수로 가지는 수라고 할 수 있다.

애초에 전제조건이 1보다 큰 자연수 중이기 때문에, 당연히 1은 소수가 아니다.

산술의 기본 정리(모든 양의 정수는 유일한 소인수 분해를 갖는다.)의 '1보다 큰 모든 자연수는 그 자체가 소수이거나, 순서를 무시하고 유일한 소인수의 조합을 갖는다'는 내용을 바탕으로 자연수는 1과 소수, 그리고 합성수로 구분된다. 

이 합성수들은 소수의 곱으로 생각할 수 있기 때문에 결국 2이상의 모든 수들은 소수들로 구성되어 있다고 볼 수 있다.

Problem Solving을 하다 보면, 소수와 관련된 문제들을 자주 접할 수 있다. 소수는 경우에 따라 변하지 않으므로 N 이하의 소수는 이미 정해져 있기 때문에, 이 이미 정해진 대량의 소수들을 빠르게 구할 수 있는 방법들이 존재한다.

그 중 대표적인 방법으로 에라토스테네스의 체가 있다. 여기서 체는 대수학에서 사용하는 유리수의 집합, 실수의 집합, 복소수의 집합을 유리수체(體), 실수체(體), 복소수체(體)라고 부르는 Field가 아닌 가루나 액체를 거를 때 사용되는 도구를 뜻한다. 

지구의 크기를 처음으로 계산해 낸 수학자로도 유명한 고대 그리스의 수학자 에라토스테네스가 만들어 낸 특정 범위 내의 소수를 구하는 방법으로, 소수가 아닌 수를 거르는 방법이 마치 체로 치듯이 수를 걸러낸다고 하여 '에라토스테네스의 체'라고 부른다고 한다. 

에라토스테네스의 체로 소수를 찾는 방법은 아래와 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/51703260/131170962-794ff277-21b4-458b-bddf-86522bd05895.gif"></p>

만약 100만 이하의 소수들을 모두 구한다고 하자.

1. 2를 제외한 2의 배수들은 모두 제거한다.
2. 3을 제외한 3의 배수들은 모두 제거한다.
3. 제거 되지 않은 가장 작은 수를 제외한 해당 수의 배수를 제거한다.
4. 100만의 제곱근인 1000까지만 체크하여 해당 수들의 배수들을 모두 제거한다.

위 과정이 끝나고 제거되지 않은 수들이 100만 이하의 소수가 된다.

여기서 중요한 점은 에라토스테네스의 체를 이용해 N 이하의 소수를 구하고 싶다면, N까지 배수들을 찾아 볼 필요는 없이 N의 제곱근까지만 체크하면 된다. 

만약 N보다 작은 합성수 M이 있을 때, M = A * B 라면 A와 B 중 적어도 하나는 N의 제곱근 보다 작다. 즉, M은 N의 제곱근 이하에서 이미 배수체크가 가능해지고 소수가 걸러진다.

같은 논리로 결국 N 또한 N의 제곱근이하에서 이미 배수체크가 완료되어 소수가 다 걸러지게 된다.

```
// 수도코드
eratosthenes(N){
	isPrime = boolean[N+1]; // 0 ~ N 까지 논리형 배열
	isPrime.fill(true); // 전부 true로 초기화 (그냥 이렇게 초기화가 가능하다고 가정)
	isPrime[0] = isPrime[1] = false; // 0과 1은 소수가 아님
	
	for(i = 2; i <= sqrt(N); i++){ // N의 제곱근 까지만 체크
		if(!isPrime[i]) continue; // 소수가 아니면 continue
		for(j = i*i; j <= N; j += i){
			isPrime[j] = false; // i의 제곱부터 시작해서 i의 배수는 모두 소수가 아님.
		}			    // i * 2부터 시작해도 되지만 어차피 i가 sqrt(N)까지 접근하기 때문에 동일함
	}
	
	return isPrime;
}

main(){
	N = 1000000;
	isPrime = eratosthenes(N);
	for(i = 0; i <= N; i++){
		if(isPrime[i]) print(i);
	}
}
```
다만 에라토스테네스의 체는 '특정 범위 내의 소수'를 구하는 데에만 효율적이다.

만약 주어진 수 하나가 소수인가? 만을 따지는 상황이라면 에라토스테네스의 체 보다 비교도 안되게 빠른방법이 넘쳐난다.

아래 문제는 에라토스테네스의 체를 이용해서 해결할 수 있는 기본적인 소수를 구하는 문제와 약간 응용한 문제들이다.

[백준 1929번 : 소수 구하기](https://www.acmicpc.net/problem/1929)

[백준 4948번 : 베르트랑 공준](https://www.acmicpc.net/problem/4948)

[백준 9020번 : 골드바흐의 추측](https://www.acmicpc.net/problem/9020)


## 유클리드 알고리즘

유클리드 알고리즘은 우리에게 유클리드 호제법이라고 더 알려져있다.

수학자 유클리드에 의해 기원전 300년경에 발견된 이 알고리즘은 주어진 두 수 사이에 존재하는 최대공약수를 구하는 알고리즘이다.

이 알고리즘의 원리는 아래와 같다.

만약 임의의 두 자연수 A와 B의 최대공약수를 구한다고 하자.

1. A를 B로 나눈 나머지 M을 구한다. M = A % B
2. 이 때 A가 B보다 작으면 M은 A가 된다. (굳이 A와 B의 대소 반별이 필요없다는 의미)
3. 만약 A가 B보다 커서 M == 0 가 된다면 B가 최대 공약수가 된다.
4. 만약 M이 0이 아니면, A에 B값을 넣고, B에 M값을 넣어서 다시 1.의 M = A % B 연산을 하여 M이 0이 될 때 까지 반복한다.

```
// 수도코드
// 재귀
euclid(A, B){	
 	return B == 0 ? A : euclid(B, A % B);
}

// 반복문
euclid(A, B){ 
	while(B != 0){
		A = B;
		B = A % B;
	}
	return A;
}
```

A와 B의 최대공약수를 구했으면, A와 B의 최소공배수는 A * B / (A와B의 최대공약수)로 구할 수 있다

아래 문제는 유클리드 호제법을 이용해서 해결할 수 있는 기본적인 문제이다.

[백준 1934번 : 최소공배수](https://www.acmicpc.net/problem/1934)

## 모듈라 연산

몇 가지 중요한 암호 시스템은 계산 결과가 항상 0 - (M-1) 범위에있는 경우 모듈라 연산을 사용한다고 한다.

이때 M이 우리가 %를 하고자 하는 모듈라 값이다.

아래는 modular를 mod를 표현한 우리가 기본적으로 알고있는 모듈라 연산이다

43 mod 6 = 1

27 mod 9 = 0

3 mod 20 = 3

50 mod 17 = 16

그리고 음수의 경우에도 모듈러 연산이 가능하다.

-13 mod 11 = 9

-10 mod 11 = 1

일반적으로 수학적으로 나머지는 양수라고 약속했기 때문에 음수를 mod 할 경우에는 양수라 생각하고 mod를 한 값의 음수에서 + m을 해주면 된다.

예를 들어 -13 mod 11이면 13 mod 11 = 2 에서 -2 + 11 = 9와 같다.

하지만 프로그래밍을 할 때 A % B 에서 A 또는 B가 음수가 되면 결과는 어떻게 될까?

놀랍게도 답은 ["구현마다 다르다(Implementation-defined)"](https://en.wikipedia.org/wiki/Modulo_operation#In_programming_languages
)

당장 파이썬에서 -10 % 4는 2가 출력되고 10 % -4는 -2가 출력된다.

그리고 C++17 -10 % 4는 -2가 출력된다. 그렇기 때문에 음수 모듈라 연산을 할 때는 언어별로 다르다는 점을 미리 고려해야 할 것 같다.

## 모듈라 합동

모듈라 연산에 이해했다면 모듈라 합동에 대해서도 알면 좋을것 같다.

__(A mod M) = (B mod M) => A ≡ B (mod M)__

어떤 값 A와 B가 M으로 나누었을 때 나머지가 같다면 A와 B는 모듈라 M에 대한 합동 관계라고 표현한다.

여기서 A와 B는 A - B를 하였을 때, M의 배수가 된다.

다시 말해 A - B = K * M (K는 임의의 정수)이다.

예를 들어 13 % 6 = 1이고, 25 % 6 = 1이므로, 13과25는 모듈라 6에 대한 합동이라고 말할 수 있다.

아래 문제는 이 모듈라 합동에 관련된 문제이다.

[백준 2981번 : 검문](https://www.acmicpc.net/problem/2981)

## 모듈라 연산의 속성

모듈라 연산에는 재밌는 속성들이 존재한다.

먼저 (A + B) mod M = ((A mod M) + (B mod M)) mod M  이 성립한다.

그리고 (A - B) mod M = ((A mod M) - (B mod M)) mod M 이 성립하며

놀랍게도 (A * B) mod M = ((A mod M) * (B mod M)) mod M 또한 성립한다.

우리는 수학자가 아니라 공학자이므로 증명은 생략하고 위 공식을 잘 써먹기만 하면 된다.

이 공식을 잘 이용한다면 아래와 같은 문제를 풀 수 있다.

> 2^50이상은 계산할 수 없는 계산기가 존재한다.
> 
> 이 계산기는 mod 연산을 할 수 있는 기능이 탑재되어있다.
> 
> 이 때 2^90 mod 13을 구하라.

이 문제는 거듭제곱을 가지는 값을 모듈라 곱셈 속성을 이용해서 분할 정복으로 해결할 수 있다.

2^90은 2^50 * 2^40이다. 그러면 2^90 mod 13은 

(A * B) mod M = ((A mod M) * (B mod M)) mod M 를 이용하여

2^90 mod 13 = (2^50 * 2^40) mod 13 = ((2^50 mod 13) * (2^40 mod 13)) mod 13으로 변형시킬 수 있다.

계산기를 통해서 2^50 mod 13 = 4, 2^40 mod 13 = 3이라는 값은 바로 구할 수 있다고 했을 때,

결국 2^90 mod 13 = 12 mod 13 = 12라는 사실을 알 수 있다.

## 이항 계수

우선 이항 계수를 다루기전에 근본적으로 이항 정리에 대해 알아야 한다.

![화면 캡처 2021-09-01 230923](https://user-images.githubusercontent.com/51703260/131686399-5fe4ea2c-cc02-431a-80f1-a243b6ebcf13.png)

이항 정리는 (A + B)^n (여기서 n은 음이 아닌 정수)라는 다항식을 전개할 때 쓰는 정리이며, 여기서 '이항'이라는 단어는 '두 개의 항'이라는 뜻이다. 

이항 계수는 (A + B)^n 라는 다항식을 전개했을 때, A^r*B^(n-r) (0 <= r <= n인 정수)의 계수를 의미한다.

A^r*B^(n-r)의 계수는 총  n개의 문자를 순서없이 배열하는 경우의 수와 같으며, 이는 조합과 같다.
			    
그래서 A^r*B^(n-r)의 계수는 nCr과 같다.

우리는 이항 계수의 수학적인 증명이나 성질을 자세하게 다루기보다는 PS에 이항 계수 관련 문제가 나오면

어떻게 이 이항 계수를 빠르고 효율적으로 구할 수 있느냐를 중점적으로 알아보려고 한다.

2가지 문제를 통해서 이항 계수 PS을 알아보도록 하자. 먼저 가장 기초적인 이항 계수를 구하는 문제이다.

> 자연수 N과 정수 K가 주어졌을 때, 이항 계수를 10,007으로 모듈라 연산한 값을 구하시오. (1 <= N <= 1,000 / 0 <= K <= N)

```
binomial_coefficient(N, K){
	K = Max(K, N-K); // K와 N-K 중 큰 수를 고른다. return으로  K를 나눈 값을 return하는데 K가 0일 수 있기 때문에
	A = N;
	
	for(i = K-1; i > 0; i--){
		A = A * (N - i); // nPk
		K = K * i;	// k!
	}
	
	return A/K; //nPk/k! == nCk
}
```
보통 위 같은 문제는 겉으로 보기에는 return 값이 엄청나게 커져 오버플로우가 발생할 수 있기 때문에 보기 편하게 10,007을 나눈 나머지 값을 구하라는 문제라고 생각할 수 있다.

그리고 실제로 N의 범위가 1,000까지기 때문에 정석적인 방식으로 이항계수를 구하고 모듈라 연산을 그냥 리턴할 때 하면 된다.

만약 N의 범위가 4,000,000이라면 위처럼 구할 수 있을까? 동일하게 자연수 N과 정수 K가 주어졌을 때, 이항 계수를 구하는 문제인데,

N의 범위를 4,000,000까지 확장하여 이항 계수를 1,000,000,007으로 모듈라 연산한 값을 구하여라.

이 땐 처음 문제와 다르게 모듈라 연산의 존재 유무가 중요해졌다.

이 문제를 풀기 위해서는 우리가 앞서 배운 모듈라 연산의 분배법칙 속성을 활용해야 한다.

N과 K의 이항계수는 nCk = N!/(K!*(N-K)!) 로 구할 수 있다.

그렇다면 이 이항계수를 모듈라 연산으로 분할정복해서 구할 순 없을까?

하지만 위에서 봤을 때, 덧셈, 뺄셈, 곱셈에 대해서는 분배법칙이 존재했지만, 나눗셈은 존재하지 않았다.

즉, (A / B) mod M != ((A mod M) / (B mod M)) mod M 이라는 것이다.

여기서 A를 N!, B를 K!*(N-K)! 이라고 대입해보자.

(N! / K!*(N-K)!) mod M != ((N! mod M) / (K!*(N-K)! mod M)) mod M 으로 바꿀 수 있다.

왜 성립하지도 않는데 귀찮게 대입을 해서 식을 직접 눈으로 확인해 본 이유가 있다.

우리가 이항 계수의 분수가 나눗셈이기 때문에 분배법칙이 적용되지 않았는데, 이 분수를 비틀어서 곱셈으로 만들어 버린다면 이항계수의 분배법칙을 가능하게 할 수 있다.

그렇다면 곱셈꼴로는 어떻게 만들까? 분수를 곱셈꼴로 만드는 방법은 역원을 이용하면 쉽게 만들 수 있다.

만약 A / B = C 라면, 이는 A * B^(-1) = C라는 의미이다. 즉 A 나누기 B는 A 곱하기 B의 역원과 같다.

그렇기 때문에, (N! / K!*(N-K)!) mod M는 (N! * (K!*(N-K)!)^(-1)) mod M 로 표현 가능하다.

역원을 이용해 나눗셈을 곱셈까지는 표현하는데는 성공하였다. 그런데 나눗셈이 존재했던 이유는 분수때문이고, 분수의 역원은 어쨌든 분수가 아닌가? 

이 문제를 해결하기 위해서 필요한 공식이 '페르마의 소정리'이다.

페르마의 소정리는 다음과 같다.

> A는 정수, P는 소수이며 A가 P로 나눠지지 않을 때, (A는 P의 배수가 아니라는 뜻)
>
> A^P ≡ A (mod P)이다. (P에 대해 모듈라 합동이다 : P를 나눈 나머지가 같다.)
 
이는 이렇게 표현이 가능하다 -> A^P mod M ≡ A mod M
 
다시 말하지만 우리는 위 공식이 어떻게 증명되는지에 대해서는 관심없고 증명된 공식들을 잘 써먹는데에 관심이 있는 공학자들이다.

위 표현식을 다시 응용하면, A^(P-1) ≡ 1 (mod P) => A * A^(P-2) ≡ 1 (mod P)로 변형이 가능하다.

놀랍게도, A (mod P)에 대한 역원은 A^(P-2) (mod P)라는 것이다.

그렇다면 다시 문제로 돌아와서 해당 분수의 역원을 페르마의 소정리로 구해보면,

A는 (K! * (N-K)!) , P는 1,000,000,007 로 대입할 수 있다.

A^(-1) = A^(P-2) = (K! * (N-K)!)^(-1) = (K! * (N-K)!)^1,000,000,005 가 된다.

이제는 더 이상 역원이 분수가 아닌 정수로 표현되니, 모듈라 곱셈 분배 법칙 적용할 수 있게 되었다.

최종적으로 도출되는 식은 아래와 같다.

  N! / (K!*(N-K)!) mod M
  
= (N! * (K!*(N-K)!)^(-1)) mod M 

= (N! * (K!*(N-K)!)^(M-2)) mod M 

= ((N! mod M) * (K! * (N-K)!)^(M-2) mod M) mod M

이렇게 정리가 된다.

이제 곱셈 분배법칙이 적용되니 분할 정복을 하여야 한다.

분할 정복은 (K! * (N-K)!)^(M-2)에서 지수 M-2를 계속 절반씩 나눠서 지수가 짝수일 때와 홀수일 때를 구분하여 리턴해주면 된다.

아래는 위 문제의 수도코드이다.
```
// 수도코드
M = 1,000,000,007
A = factorial(N); // N!
B = factorial(K) * factorial(N-K) % M; // K!*(N-K)!

print(A * divide_conquer(B, M-2) % M); // (N! * (K!*(N-K)!)^(M-2)) mod M  

// 팩토리얼 구하면서 mod M을 계속 해줌
factorial(num){
	result = 1;
	while(num > 1){
		result = (result * num--) % M
	}
	
	return result;
}

// num : 밑수, exp : 지수
divide_conquer(num, exp){
    	if(exp == 1) return num % M; // 지수가 1일 경우 num^1 이므로 num % M 리턴
    	
	temp = divide_conquer(num, exp/2); // 모듈라 연산 곱셈 분배법칙을 이용한 분할 정복
	
	if(exp % 2 == 1) return (temp * temp) * num % M; // 분할 정복이 끝나고 지수가 홀수가 남으면 ex)A^5 = A^2 * A^2 * A
	else return temp * temp % M; // 지수가 짝수면 A^4 = A^2 * A^2
}
```

<br>

# 기초 자료구조

## 비트마스크

정수의 이진수 표현을 자료 구조로 쓰는 기법

#### 장점
- **빠른 수행 시간**
    비트마스크 연산은 **O(1)**에 구현되는 것이 많다.
    비트마스크를 사용할 수 있다는 말은 원소의 수가 많지 않다는 뜻이어서 큰 속도 향상은 없지만, 연산을 여러 번 수행해야하는 경우에는 이러한 최적화로 큰 속도 향상을 가져올 수 있다.
- **간결한 코드**
    집합 연산들을 반복문 없이 비트 연산으로 처리하기 때문에 코드의 길이가 간결해진다.
- **더 작은 메모리 사용량**
    비트마스크를 사용하면 같은 데이터를 더 적은 메모리를 사용해 표현할 수 있다.
- **배열을 비트마스크로 대체**
    불린 값을 갖는 배열을 비트마스크를 사용해 같은 정보를 정수 변수로 나타낼 수 있다.

#### 비트연산 시 유의점
**연산자 우선순위**
Java 에서는 `&` , `|` , `^` 등의 비트 연산자의 우선순위는 `==` , `!=` 등의 비교 연산자보다 낮다.

```java
int c = (6 & 4 == 4);
```
위의 코드는 `4==4` 가 먼저 계산되고 이 결과인 `1` 이 `6` 과 비트 연산이 되어서 `6 & 1` 이 된다.

```java
int c = ((6 & 4) == 4);
```
그래서 괄호로 감싸줘야 하며, **비트마스크를 사용할 때는 괄호 사용을 습관화하는 것이 좋다.**

**부호 있는 정수형의 사용**
부호 있는 정수형에서는 최상위 비트가 음수/양수를 표현한다.
32비트 정수형에서 하위 비트로만 사용할 경우는 문제가 되지 않지만, 32비트를 전부 사용하고 싶다면 음수의 경우 버그가 생길 수 있다.
따라서 **변수의 모든 비트를 사용해 비트마스킹을 하고싶다면 부호 없는 정수형을 쓰거나 크기가 더 큰 정수를 사용하는 것이 좋다.**
Java의 경우 `unsigned int` 가 없기 때문에 `long` 을 사용해야 한다.

### 비트마스크를 활용한 집합
**비트마스크**로 표현하면 **N비트 정수 변수는 0부터 N-1까지의 정수 원소를 가질 수 있는 집합**이 된다.
이때 원소 i가 집합에 속해 있는지 여부는 정수 변수의 i번째 비트가 1인지 0인지로 나타낸다.

<br/>

#### 공집합과 꽉 찬 집합

비트마스크에서 집합을 표현할 때, **0이 공집합**을 나타낸다.
**꽉찬 집합**은 마지막 N개의 비트가 모두 1인 것인 것인데, 이것은 **`(1 << N) - 1`** 로 표현 가능하다.
`1 << N` 은 1뒤에 N개의 0이 있는 정수인데, 여기서 1을 뺀다면 N자리부터 끝까지 모두 1이 된다.

**예시**

8비트 정수 변수와 [0, 1, 2, 3, 4 , 5] 의 6개의 원소가 있다.

- 공집합

| 0    | 0    | 0    | 0    | 0    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- 꽉 찬 집합

| 0    | 0    | 0    | 0    | 0    | 0    | 0    | 1    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

| 0    | 1    | 0    | 0    | 0    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

| 0    | 0    | 1    | 1    | 1    | 1    | 1    | 1    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

[0, 1, 2, 3, 4, 5] 의 꽉 찬 집합

<br/>

#### 집합에 원소 추가

비트마스크에서 원소를 추가한다는 것은 해당 자리의 비트를 1로 만드는 것이다.
**기존 값이 `result` 라고 했을 때, 원소 `p` 를 추가하는 것은 `result |= (1 << p)` 로 나타낼 수 있다.**
1을 왼쪽으로 `p` 비트 시프트하면 `p` 번 비트만 1인 정수가 되므로 이것을 `result` 와 비트 OR 연산을 한다면 `p` 번 비트를 0에서 1로 변경할 수 있다.

**예시**

[1, 3, 4] 에 2 추가

- [1, 3, 4] 집합을 뜻하는 변수의 이진법 상태

| 0    | 0    | 0    | 1    | 1    | 0    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- 1 << 2

| 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- [1, 3, 4] OR (1 << 2)

| 0    | 0    | 0    | 1    | 1    | 1    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

[1, 2, 3, 4] 집합을 뜻하는 변수 상태 완성

<br/>

#### 원소 포함 여부 확인

**`result` 에 `p` 번 원소가 포함되어있는지 확인하려면 `result & (1 << p) != 0` 의 조건으로 확인**하면 된다.
`result` 의 `p` 번째 비트가 0이라면 `&` 연산에 의해 0이 도출되므로, 0이 아니라면 `p` 번 원소가 포함되어있는 것이다.
여기서 주의할 점은 `&` 의 연산 결과는 1이 아닌 0 또는 `1 << p` 값이다.



**예시**

[1, 3, 4] 집합에 3이 포함되어있는지 여부

- [1, 3, 4] 집합을 뜻하는 변수의 이진법 상태

| 0    | 0    | 0    | 1    | 1    | 0    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- 1 << 3

| 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- [1, 3, 4] AND (1 << 3)

| 0    | 0    | 0    | 0    | 1    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

결과가 0이 아니므로 해당 원소는 집합에 포함된 상태

<br/>

#### 원소의 삭제
삭제하는 방법 중 단순하게 `result -= (1 << p)` 로 할 수도 있다.
하지만 위의 경우는 **이미 `result` 에 `p` 번째 원소가 포함되어 있는 경우에만 유효**하고, 만약 `result` 에 `p` 번째 원소가 포함되어있지 않은 경우에는 버그가 발생한다.

정상 동작하도록 할거라면, **`result &= ~(1 << p)`** 로 해야한다.
`~` 연산자는 비트별 NOT 연산을 수행하므로 `~(1 << p)` 는 `p` 번째 비트만 0이고 나머지는 모두 1이 된다.
따라서 `result` 에서 `p` 번째 원소만 0으로 만들고, 나머지는 그대로 유지할 수 있게 된다.



**예시**

[1, 3, 4] 집합에서 언소 4 삭제

- [1, 3, 4] 집합을 뜻하는 변수의 이진법 상태

| 0    | 0    | 0    | 1    | 1    | 0    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- 1 << 4

| 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- ~(1 << 4)

| 1    | 1    | 1    | 0    | 1    | 1    | 1    | 1    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- [1, 3, 4] AND ~(1 << 4)

| 0    | 0    | 0    | 0    | 1    | 0    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

[1, 3] 만 포함하는 변수 상태 완성

<br/>

#### 원소의 토글
비트 토글은 **XOR 연산**을 활용한다.
만약 **`p` 번이 포함된 경우는 제외하고, 제외된 경우는 포함하고 싶다면 `result ^= (1 << p)` 를 사용하면 된다.**



**예시**

[1, 3, 4] 집합에서 원소 4의 토글

- [1, 3, 4] 집합을 뜻하는 변수의 이진법 상태

| 0    | 0    | 0    | 1    | 1    | 0    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- 1 << 4

| 0    | 0    | 0    | 1    | 0    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- [1, 3, 4] XOR (1 << 4)

| 0    | 0    | 0    | 0    | 1    | 0    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

[1, 3] 만 포함하는 변수 상태 완성

<br/>

#### 두 집합 연산
- **합집합** : `a | b`
- **교집합** : `a & b`
- **a에서 b를 뺀 차집합** : `a & ~b`
- **합집합에서 교집합을 뺀 집합** : `a ^ b`

위 연산은 원소 하나에 대해 수행하는 것과 다를 것이 없다.
즉, 집합 간의 연산 속도가 굉장히 빨라진다.
<br/>

#### 집합 크기 구하기
비트마스크를 이용할 때 집합에 포함된 원소의 수를 구하는 방법은 딱히 없다.
가장 간단한 방법은 **각 비트를 순회하면서 켜져 있는 비트의 수를 직접 세는 수밖에 없다.**
```java
int bitCount(int x) {
	if (x == 0) return 0;
	return x % 2 + bitCount(x/2);
}
```
위의 방법 말고도, Java는 `Integer.bitCount(result)` 를 통해 1인 비트의 수를 셀 수 있다.

<br/>

#### 최소 원소 찾기
최소 원소를 찾는다는 것은, **1비트인 최하위 비트의 위치를 구하는 방법**이다.
Java의 `Integer.numberOfTrailingZeros(result)` 를 활용하는 방법도 있다.

만약 최하위 비트의 번호 대신 해당 비트를 직접 구하고 싶다면 **`result & -result` 를 사용**하면 된다.
대부분의 컴퓨터가 음수를 표현하는 것에 2의 보수를 사용한다는 점을 이용한 방법이다.
컴퓨터는 `-result` 를 표현하기 위해서 `result` 에 비트별로 NOT 연산을 적용한 결과에 1을 더한다.
만약 `result` 에서 0비트가 아닌 최하위 비트가 2^i 라면, `result` 의 마지막 i+1 자리는 1 뒤에 i개의 0이 있는 형태여야 한다.
`result` 에 비트별 NOT 연산을 적용하면 i+1 자리는 0이되고 0 뒤에 i개의 1이 있는 형태가 되고, 여기에 1을 더하면 다시 1과 i개의 0이 있는 형태가 된다.
2^i보다 상위 비트들에는 NOT 연산이 적용된 상태이므로 두 수를 AND 연산하면 항상 최하위 비트만 얻을 수 있다.



**예시**

[2, 3, 4] 원소들을 포함하는 집합을 뜻하는 8비트 정수 변수에서 최소 원소값을 찾기

- `reseult`

| 0    | 0    | 0    | 1    | 1    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- ` -result`

| 1    | 1    | 1    | 0    | 0    | 0    | 1    | 1    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

| 1    | 1    | 1    | 0    | 0    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- `result & -result`

| 0    | 0    | 0    | 0    | 0    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

2를 뜻하는 결과값 도출

<br/>

#### 최소 원소 지우기
만약 최소 원소가 무엇인지는 궁금하지 않고 **무조건 최소 원소를 지우는 연산은 `result &= (result -1)`** 을 활용한다.
최소원소를 얻은 후 그 원소를 지우는 것보다 훨씬 간결하다.
`result-1` 의 이진수 표현은 최하위 비트를 0으로 만들고 하위 비트들을 모두 1로 만든 것이다.
따라서 두 값을 AND 연산한다면 최하위 비트와 그 이하의 비트들은 전부 0이 되므로 최소 원소를 지우는 것과 같은 효과가 나타난다.



**예시**

[2, 3, 4] 원소들을 포함하는 집합을 뜻하는 8비트 정수 변수에서 최소 원소 지우기

- `result`

| 0    | 0    | 0    | 1    | 1    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- `result-1`

| 0    | 0    | 0    | 1    | 1    | 0    | 1    | 1    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- `result & (result-1)`

| 0    | 0    | 0    | 1    | 1    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

2 원소를 삭제한 결과값 도출

<br/>

#### 모든 부분 집합 순회하기
```java
for(int subset = result; subset != 0; subset = ((subset-1) & result)) {
	// subset은 부분집합
}
```
위와 같은 **반복문으로 부분 집합을 순회**할 수 있다.
**`subset-1` 은 최하위 비트가 꺼지고 그 밑의 비트들은 모두 켜진다.
이 값에 `& result` 를 하게되면 그 중 `result` 에 속하지 않는 비트들은 모두 0이 된다.**
이 연산을 반복하면 `result` 의 모든 부분 집합을 방문할 수 있다.
이 때, for문의 종료 조건이 `subset` 이 0이 아닐때까지이므로 공집합은 따로 체크해야한다는 점을 유의해야 한다.



**예시**

[2, 3, 4] 원소들을 포함하는 집합을 뜻하는 8비트 정수 변수에서 부분집합 구하기

- `result` = 처음 `subset`

| 0    | 0    | 0    | 1    | 1    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- `subset-1`

| 0    | 0    | 0    | 1    | 1    | 0    | 1    | 1    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- `(subset-1) & result`  = 다음 `subset`

| 0    | 0    | 0    | 1    | 1    | 0    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

[3, 4] 원소들을 포함하는 집합

- `subset-1`

| 0    | 0    | 0    | 1    | 0    | 1    | 1    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

- `(subset-1) & result` = 세번째 `subset`

| 0    | 0    | 0    | 1    | 0    | 1    | 0    | 0    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |

[2, 4] 원소들을 포함하는 집합

...

<br/>

### 비트마스크 활용 방법
- **메모이제이션 또는 visit 배열**
- **2의 제곱들을 활용하는 문제**

<br/>

#### 백준에서는..
[실버1 물병 ](https://www.acmicpc.net/problem/1052)
[골드3 중복제거](https://www.acmicpc.net/problem/13701)
[골드3 IP 주소](https://www.acmicpc.net/problem/2064)

<hr/>

## 부분 합
**부분합**이란 배열의 **각 위치에 대해서 배열의 시작부터 현재 위치까지의 원소의 합을 구해 둔 배열**이다.
만약 부분합을 미리 계산해둔다면, 특정 구간의 합을 **O(1)**에 구할 수 있다.

### 부분 합 계산하기
구간 합을 빠르게 계산하기 위해 부분 합을 미리 계산해 둘 필요가 있다.
**부분 합을 계산하는데 드는 시간은 수열의 길이에 따라 선형으로 증가한다는 것에 유의**해야 한다.
반복문을 통해 구간 합을 구하기 위해 최대 O(N)의 시간이 걸린다는 것은 구간 합을 두 번 이상 구할 때는 대부분의 경우 부분 합을 미리 계산해놓고 사용하는 쪽이 효율적이다.

### 2차원으로의 확장
![image](https://user-images.githubusercontent.com/49138331/131240703-018357bb-df7b-4166-a7ea-152eda19af0b.png)
```java
sum[i+1][j+1] = sum[i][j+1] + sum[i+1][j] - sum[i][j] + arr[i][j]
```
**2차원 배열에서 구간 합 배열을 구하는 식**은 위와 같다.
위와 같이 구한다면, **배열의 (0,0) 위치부터 (i+1,j+1) 위치까지의 합을 저장**할 수 있다.

만약 **(x1, y1) 부터 (x2, y2) 구간의 합을 구하는 방법**은 아래와 같다.
```java
arr[x2][y2] - arr[x1-1][y2] - arr[x2][y1-1] + arr[x1-1][y1-1]
```
위의 코드로 해당 영역의 합을 구할 수 있다.

#### 백준에서는...
[골드1 구간 합 구하기](https://www.acmicpc.net/problem/2042)
[골드2 구간 합 최대](https://www.acmicpc.net/problem/14706)

## 선형 자료구조

<img src="https://user-images.githubusercontent.com/53392870/131146547-12f8c43f-4d66-47d9-8721-1fc924084ddb.png" width="50%">
출처: https://lee-mandu.tistory.com/462

- 자료들간의 관계가 **1:1 선형 관계**를 가진다.
- 즉, 하나의 자료 뒤에 다른 하나의 자료가 존재한다.
- 선형 리스트, 연결 리스트, 큐, 스택, 데크

## 선형 리스트

![image](https://user-images.githubusercontent.com/53392870/131146010-f4c1f481-90f0-477b-8c3b-fa40ba30c6fb.png)

### 특징
- 배열 기반으로 구현되어 연속되는 메모리 공간에 저장되는 리스트
- 인덱스 번호를 이용해서 매우 빠르게 접근할 수 있다.
- 메모리 공간으로 연속적으로 배정받기 때문에 메모리 공간 사용 효율이 좋다.
- 삽입/삭제 연산 과정에서 연속되는 메모리 배열을 위해 원소들의 이동이 필요하기 때문에 작업이 번거롭다.
- 원소의 개수가 많고 삽입/삭제 연산이 빈번하게 일어날수록 작업에 소요되는 시간이 크게 증가한다.

## 연결 리스트
 
![image](https://user-images.githubusercontent.com/53392870/131145206-ff2a9346-46fb-4df7-8419-eef0cd3dbd3a.png)
출처: https://lipcoder.tistory.com/entry/%EB%A6%AC%EC%8A%A4%ED%8A%B8

### 특징
- 메모리의 동적할당을 기반으로 구현되었고, 노드의 링크 필드 속성에 다음 노드에 대한 참조값을 저장함으로써 노드와 노드가 연결 구조를 가지는 리스트
- 순차 리스트와 달리 연속적인 메모리 공간에 원소가 저장되지 않는다.
- 따라서 자료의 논리적 순서와 메모리 상의 물리적인 순서가 일치하지 않으며 물리적인 순서를 맞추기 위한 작업이 필요하지 않다.

### 시간복잡도
#### 탐색 O(n)
- 특정 원소의 검색/수정 연산 시 데이터를 찾기 위한 탐색 작업이 필요하다. ( 시간복잡도 -> O(n) )
#### 삽입/삭제 O(1)
- 삽입/삭제 연산 자체는 O(1)이다.
- 맨 처음에 원소를 삽입/삭제하는 경우 탐색 작업이 불필요하기 때문에 시간복잡도는 O(1)이다.
- 리스트 중간에 원소를 삽입하거나 원소를 삭제하는 경우 탐색 작업이 추가된다. 

### 기본 구조
#### 노드
- 데이터 필드
원소값을 저장한다.
저장하고자 하는 데이터의 특징에 따라 정의해서 사용한다.
- 링크 필드
연결 리스트의 첫 노드에 대한 참조값을 가지고 있다.

### 종류
링크의 개수에 따라 연결 리스트를 나눌 수 있다.
#### 단일 연결 리스트 ( Singly Linked List )
- 링크 개수: 1개
#### 양방향 연결 리스트 ( Doubly Linked List )
- 링크 개수: 2개
- 링크가 2개이기 때문에 양쪽 방향으로 순회할 수 있다.
#### 원형 연결 리스트 ( Circular Linked List )
- 링크 개수: 1개 혹은 2개
- 단일 연결 리스트에서 마지막 노드와 처음 노드를 연결시켜 원형으로 만든 구조
- 어떤 노드에서 출발해도 모든 노드로 접근할 수 있다.

### 링크 1개와 링크 2개의 장단점
#### 링크 1개 ( 단일 연결 리스트 )
- 장점
링크가 1개이기 때문에 연결 리스트의 관리가 수월하다.
- 단점
삭제 연산 시 삭제하고자 하는 노드의 이전 노드를 기억해야 한다.
따라서 head에서부터 삭제할 노드와 그 이전 노드를 찾아야 한다.

#### 링크가 2개 ( 양방향 연결 리스트 )
- 장점
2개의 링크가 이전 노드와 다음 노드의 참조값을 기억하고 있기 때문에 단일 연결 리스트의 단점을 극복할 수 있다.
삭제 연산 시 삭제할 노드만 찾으면 삭제할 수 있다.
- 단점
링크가 2개이기 때문에 연결 리스트의 관리가 복잡하다.

## 큐와 스택과 데크
## 큐

<img src="https://user-images.githubusercontent.com/53392870/131144364-fab07492-1de5-4ac8-bbc8-b10df285175d.png" width="50%">

출처: https://galid1.tistory.com/483

### 특징
- 한쪽에서는 삽입, 다른 한쪽에서는 삭제 작업이 이루어지도록 구성한 자료구조
- FIFO ( First-In-First-Out): 가장 먼저 삽입된 자료가 가장 먼저 삭제된다.

### 연산
- enqueue ( 삽입 )
자료를 큐에 삽입한다.
- dequeue ( 삭제 )
큐에서 자료를 삭제한다.
FIFO에 따라 제일 먼저 삽입했던 자료가 삭제된다.

## 스택
<img src="https://user-images.githubusercontent.com/53392870/131144029-20e8508f-aeda-4ff0-9527-1571fd895f11.png" width="50%">

### 특징
- 리스트의 한쪽 끝으로만 자료의 삽입, 삭제가 이루어진다.
- LIFO ( Last-In-First-Out): 가장 나중에 삽입된 자료가 가장 먼저 삭제된다.

### 연산
- push ( 삽입 )
자료를 스택에 삽입한다.
- pop ( 삭제 )
스택에서 자료를 꺼낸다.
LIFO에 따라 제일 최근에 삽입된 자료가 가장 먼저 삭제된다.
- peek ( 맨 위의 자료 반환 )
스택의 top에 있는 자료를 반환한다.

## 데크

![image](https://user-images.githubusercontent.com/53392870/131146203-e7333b08-2ec7-4b1b-88cc-3bc38f08260e.png)
출처: https://jjudrgn.tistory.com/15

### 특징
- 삽입과 삭제가 리스트의 양쪽 끝에서 모두 발생할 수 있는 자료구조
- 큐와 스택의 특징이 모두 포함되어 있다.

<hr/>

# 트리

# 트리의 구현과 순회

## 트리(Tree)

**계층적인 구조**의 자료를 표현하는 **비선형 자료구조(1:n)**

뿌리부터 잎까지의 나무를 거꾸로 뒤집어 놓은 형태를 띄며, 회사의 조직도, 파일 디렉토리 구조, 기계 학습에서의 결정 트리(decision tree) 등이 트리 구조로 표현될 수 있다.

## 트리 관련 용어

- 노드(node)와 간선(edge) : 트리의 구성 요소, 트리의 각 노드에는 데이터가 저장되고 이러한 데이터들의 연결 관계는 간선으로 나타낸다.
    - 루트(root) : 계층적인 구조에서 가장 높은 곳에 있는 노드. 한 트리에는 하나의 루트만이 존재한다.
    - 리프(leaf, 단말 노드) : 자식 노드가 존재하지 않는, 더 이상 아래로 뻗어나갈 수 없는 노드
    - 부모(parent) : 현재 노드에서 간선으로 연결된 위쪽에 있는 노드. 트리의 각 노드는 무조건 1개의 부모 노드만 가질 수 있다.
    - 자식(child) : 현재 노드에서 가지로 연결된 아래쪽 노드. 트리의 각 노드는 자식 노드를 가지지 않을 수도, 여러개의 자식 노드를 가질 수도 있다.
    - 형제(sibling) : 부모 노드가 동일한 노드들
    - 조상(ancestor) : 현재 노드에서 간선을 따라 루트노드까지 올라갈 때 연결된 모든 노드
    - 자손(descendant) : 서브 트리에 있는 하위 레벨의 노드들
- 서브 트리(subtree) : 부모 노드와 연결된 간선을 끊으면 새롭게 생성되는 트리. 하나의 트리는 여러개의 서브 트리를 포함하고 있다.

<p align="center"><img src="https://github.com/hongcheol/CS-study/blob/main/Algorithm/img/tree-degree-level.png?raw=true" alt="Tree degree and level" width="500"></p>

- 레벨(level)과 높이(height) : 루트 노드로부터 현재 노드에 이르기까지 연결된 간선의 수. 트리의 레벨의 최댓값이 트리의 높이가 된다.
- 차수(degree) : 현재 노드에 연결된 자식 노드의 수. 트리의 차수는 트리의 차수의 최댓값이다.

## 이진 트리(Binary Tree)

모든 노드가 2개의 서브 트리를 가지고 있는 트리.

## 이진 트리의 특징 

- 모든 노드는 왼쪽 자식 노드와 오른쪽 자식 노드를 각각 1개씩, 최대 2개의 노드만을 자식 노드로 가질 수 있다.(최대 차수 2)
- 공집합도 이진트리이다.
- 서브 트리 간 순서(왼쪽, 오른쪽)가 존재한다.
- 노드의 개수가 `n`개인 이진트리의 간선 개수는 `n-1`개
- 높이가 h인 이진트리의 최소 노드 개수는 `h+1`개, 최대 노드 개수는 `2^(h+1) - 1`개

## 이진 트리의 종류

![Binary Tree](https://github.com/hongcheol/CS-study/blob/main/Algorithm/img/binary-tree.png?raw=true)

### 포화 이진 트리(Full Binary Tree)

- 모든 높이에 노드가 최대로 차 있는 이진 트리
- 따라서 노드 개수는 이진 트리의 최대 노드 개수인 `2^(h+1) - 1`개
- 루트 노드를 1번으로 하여 모든 노드가 순서대로 노드 번호를 가진다.

### 완전 이진 트리(Complete Binary Tree)

- 루트 노드를 1번으로 하여 n개의 노드를 갖는 이진 트리에서 1번부터 n번까지 빈 자리가 없는 이진 트리
- 낮은 높이에서 시작하여 왼쪽 자식 노드부터 채워야 한다.

### 편향 이진 트리(Skewed Binary Tree)

- 높이 h에 대한 최소 개수의 노드를 가지며 한 쪽 방향의 자식 노드만을 가진 이진 트리
- 따라서 노드 개수는 이진 트리의 최소 노드 개수인 `h+1`개

## 이진 트리 표현

### 배열을 이용한 표현

n개의 노드를 갖는 이진 트리의 경우 루트 노드부터 마지막 노드까지 1번 ~ n번 번호를 매긴 뒤, 배열의 1번 인덱스부터 n번 인덱스까지 순서대로 노드를 삽입하여 구현

- 높이가 h인 이진 트리를 구현하기 위해서는 배열의 크기가 `2^(h+1)`가 되어야 한다.
    - 배열의 1번 인덱스부터 사용하므로 이진 트리의 최대 노드 개수인 `2^(h+1) - 1`개보다 1개 많은 크기
- 배열을 이용한 이진 트리는 구현이 쉽지만, 배열 원소에 대한 메모리 공간 낭비가 발생할 수 있다.
    - 편향 이진 트리의 경우 사용하지 않는 배열의 영역이 많아진다.
- 트리의 중간에 새로운 노드를 삽입하거나 삭제할 경우 배열의 크기 변경이 어려워 비효율적이다.

#### Java를 이용한 완전 이진 트리 구현 - 배열 표현법

```java
// 완전 이진 트리
class CompleteBinaryTreeArray {
    char[] nodes;         // 트리의 노드를 저장한 배열
    final int SIZE;             // 노드 개수 + 1
    int lastIndex;        // 마지막에 추가된 노드의 인덱스

    public CompleteBinaryTreeArray(int size) {
        this.SIZE = size;
        nodes = new char[SIZE+1];          // 1 ~ SIZE 번의 노드들을 저장하는 배열
    }

    public void add(char c) {
        if (lastIndex == SIZE) return;     // 배열 포화 상태
        nodes[++latIndex] = c;             // 배열에 노드 추가
    }
}
```

### 링크를 이용한 표현

이진 트리 노드 번호 성질을 이용해 각 부모 노드에 왼쪽 자식 노드와 오른쪽 자식 노드를 연결하여 표현한다. 한 노드가 두 개의 링크 필드를 갖는 형태이다.

> 💡 이진 트리 노드 번호의 성질
>- 노드 번호가 i인 노드의 부모 노드 번호 : `i/2`
>- 노드 번호가 i인 노드의 왼쪽 자식 노드 번호 : `2*i`
>- 노드 번호가 i인 노드의 오른쪽 자식 노드 번호 : `2*i + 1`
>- 레벨 n인 노드의 시작 번호 : `2^n`

- 배열 표현법의 한계인 메모리 낭비를 막을 수 있다. 이진 트리의 노드 개수만큼 노드를 생성하면 된다.
- 특정 노드를 탐색하기 위해 루트 노드부터 탐색을 시작해야 하므로 탐색이 비효율적이다.

#### Java를 이용한 완전 이진 트리 구현 - 링크 표현법

```java
// 트리의 노드
class Node {
    char data;         // 데이터 필드
    Node left;         // 왼쪽 자식 노드 링크 필드
    Node right;        // 오른쪽 자식 노드 링크 필드

    public Node(char data) {
        this.data = data;
        left = null;             // 리프 노드를 위한 초기화
        right = null;            // 리프 노드를 위한 초기화
    }
}

// 완전 이진 트리
class LinkedCompleteBinaryTree {
    Node[] binaryTree;

    public LinkedCompleteBinaryTree(int nodeCnt) {
        binaryTree = new Node[nodeCnt+1];                 // 노드 개수 + 1
    }

    public void add(int nodeCnt) {
        for (int i=1; i<=nodeCnt; i++) {
            binaryTree[i] = new Node(i);                  // 노드 생성 및 데이터 삽입
        }

        for (int i=1; i<=nodeCnt/2; i++) {
            binaryTree[i].left = binaryTree[2*i];         // 왼쪽 자식 노드 연결
            binaryTree[i].right = binaryTree[2*i+1];      // 오늘쪽 자식 노드 연결
        }
    }
}
```

## 트리의 탐색

비선형 자료구조인 트리에서 각 노드를 중복되지 않게 완전 탐색하기 위한 기법

### 너비 우선 탐색(BFS, Breadth First Search)

- 루트 노드의 자식 노드들을 우선적으로 모두 방문한 뒤, 방문했던 자식 노드들의 자식노드들을 또 다시 차례대로 방문하는 방식
- 인접한 노드에 대한 탐색이 끝나야 해당 노드들의 인접한 노드를 방문할 수 있으므로 **선입 선출 형태의 자료구조인 큐**(Queue)를 사용하여 구현할 수 있음

```java
public void bfs() {
    int lastIndex;                              // 노드의 개수 + 1
    Queue<Integer> q = new LinkedList<>();      // 탐색을 기다리는 노드를 저장할 큐
    q.offer(1);                 // 루트 노드의 인덱스

    int level = 0, size = 0;

    while(!q.isEmpty()) {
        size = q.size();        // 현재 높이(너비)에서의 모든 노드 개수

        while(size-->0) {       // 높이 별 노드들을 단계적으로 탐색
            int current = q.poll();
            
            System.out.println(current + " ");

            // 왼쪽 자식 노드 유효성 검사 후 큐에 삽입
            if (current*2 <= lastIndex) q.offer(current*2);
            // 오른쪽 자식 노드 유효성 검사 후 큐에 삽입
            if (current*2+1 <= lastIndex) q.offer(current*2+1);
        }

        level++;     // 현재 높이, 한 높이(너비) 별 탐색이 끝나면 크기 하나씩 증가
    }
}
```

### 깊이 우선 탐색(DFS, Depth First Search)

- 루트 노드에서 출발하여 한 방향으로 갈 수 있는 경로가 존재할 때까지 계속해서 깊이 탐색. 더 이상 방문할 수 있는 경로가 없으면 마지막으로 만났던 갈림길이 있던 노드로 돌아와 다른 방향의 노드를 같은 방식으로 계속해서 깊게 탐색하는 방식
- 자식의 자식의 자식 노드까지 깊게 탐색했다가 방문할 노드가 없을 경우 돌아와서 다른 자식 노드로 깊게 들어가야 하므로 **재귀**적으로 구현하거나, **후입 선출 형태의 자료구조인 스택**(Stack)을 사용하여 구현할 수 있음

```java
public void dfs(int current) {
    System.out.println(current + " ");      // 전위 순회 dfs로, 중위, 후위 순위의 경우 현재 라인의 위치만 자식 노드 중간과 마지막으로 바꿔주면 됨

    // 왼쪽 자식 노드 유효성 검사 후 재귀적 탐색
    if (current*2 <= lastIndex) dfs(current*2);
    // 오른쪽 자식 노드 유효성 검사 후 재귀적 탐색
    if (current*2+1 <= lastIndex) dfs(current*2+1);
}
```

## 트리의 순회

깊이 우선 탐색 시 트리의 노드를 순회하는 순서

### 전위 순회(preorder traversal) : VLR

노드 방문 -> 왼쪽 자식 -> 오른쪽 자식

```java
public void dfsByPreOrder() {
    System.out.print("Preorder : ");
    dfsByPreOrder(1);
    System.out.println();
}
	
private void dfsByPreOrder(int current) {
    // 현재 노드 처리
    System.out.print(nodes[current] + " ");
    // 왼쪽 자식 노드 방문
    if (current*2<=lastIndex) dfsByPreOrder(current*2);
    // 오른쪽 자식 노드 방문
    if (current*2+1<=lastIndex) dfsByPreOrder(current*2+1);
}
```

### 중위 순회(inorder traversal) : LVR

왼쪽 자식 -> 노드 방문 -> 오른쪽 자식

```java
public void dfsByInOrder() {
    System.out.print("Inorder : ");
    dfsByInOrder(1);
    System.out.println();
}

private void dfsByInOrder(int current) {
    // 왼쪽 자식 노드 방문
    if (current*2<=lastIndex) dfsByInOrder(current*2);
    // 현재 노드 처리
    System.out.print(nodes[current] + " ");
    // 오른쪽 자식 노드 방문
    if (current*2+1<=lastIndex) dfsByInOrder(current*2+1);
}
```

### 후위 순회(postorder traversal) :LRV

왼쪽 자식 -> 오른쪽 자식 -> 노드 방문

```java
public void dfsByPostOrder() {
    System.out.print("Postorder : ");
    dfsByPostOrder(1);
    System.out.println();
}

private void dfsByPostOrder(int current) {
    // 왼쪽 자식 노드 방문
    if (current*2<=lastIndex) dfsByPostOrder(current*2);
    // 오른쪽 자식 노드 방문
    if (current*2+1<=lastIndex) dfsByPostOrder(current*2+1);
    // 현재 노드 처리
    System.out.print(nodes[current] + " ");
}
```

## PS 문제 추천

[Baekjoon Online Judge > 트리의 부모 찾기](https://www.acmicpc.net/problem/11725)
[2019 KAKAO BLIND RECRUITMENT > 길 찾기 게임](https://www.welcomekakao.com/learn/courses/30/lessons/42892)


<br>

# 이진 탐색트리

이진 탐색 트리(BST, Binary Search Tree)는 이진 트리 기반의 탐색을 위해 특정한 조건들로 이루어진 이진 트리입니다.

<p align="center"><img src="./img/BST.png" width="400"></p>



1. 모든 노드의 키(Key)는 유일하다

   - Key는 노드에 기록한 데이터 값을 말합니다. 중복된 데이터 값을 갖는 노드가 없어야 합니다.

2. 왼쪽 서브 트리의 키들은 루트의 키보다 작다.

   - 루트 노드의 데이터가 10이라면, 왼쪽 서브트리에는 10보다 작은 값들만 존재해야 합니다.

3. 오른쪽 서브 트리의 키들은 루트의 키보다 크다.

   - 마찬가지로 오른쪽 서브트리에는 10보다 큰 값들만 존재해야 합니다.

4. 왼쪽과 오른쪽 서브 트리도 이진 탐색 트리이다.

   - 1~3의 조건들이 각 서브트리에 순환적으로 적용되어야 합니다.

   

이진 탐색 트리는 이진 암호화, 파일 시스템에 주로 쓰입니다.



## 이진 탐색트리 구현

### 검색

이진 탐색트리에서 60을 찾는 과정은 다음과 같습니다.

1. 루트부터 탐색을 시작합니다.
 	2. 목표 값과 현재 루트의 데이터를 비교합니다. 목표 값 < 현재 값이면 왼쪽, 반대면 오른쪽으로 이동합니다.
 	3. 일치하는 값을 찾으면 탐색을 멈춥니다.
 	4. 만약 목표 값이 없다면 null을 리턴합니다.

<p align="center"><img src="./img/BST_search.png" width="600"></p>

```java
public Node findNode(int key) {
    // 트리가 비었을 때
    if (root == null) return null;

    Node focusNode = root;

    while (focusNode.key != key) {
        if (key < focusNode.key) {              // 현재노드보다 작으면
            focusNode = focusNode.leftChild;    // 왼쪽으로
        } else {                                // 크면
            focusNode = focusNode.rightChild;   // 오른쪽으로
        }

        // 찾으려는 노드가 없을 때
        if (focusNode == null)
            return null;
    }

    return focusNode;
}
```



### 삽입

이진 탐색트리에 값을 삽입할 때 삽입할 위치를 찾는 과정은 값을 검색하는 과정과 유사하게 진행됩니다. 10을 삽입하는 과정은 다음과 같습니다.

1. 루트에서 시작합니다.
2. 삽입 값을 현재 루트의 값과 비교합니다.  삽입 값 < 현재 값이면 왼쪽, 반대면 오른쪽으로 이동합니다.
3. null을 만나면 그 이전 루트에 연결하여 값을 삽입합니다.

값을 검색하는 과정에서 null값을 만났을 때 null을 리턴 했다면, 삽입할 때는 그 곳에 값을 삽입하게됩니다.

<p align="center"><img src="./img/BST_insert.png" width="600"></p>

```java
public void addNode(int key) {
    if (findNode(key) != null) return;  // 이미 존재하면 그냥 리턴

    Node newNode = new Node(key);

    if (root == null) {
        root = newNode; // 트리가 비어있으면 root 에 삽입
    } else {
        Node focusNode = root;  //  탐색용 노드
        Node parent;            //  탐색용 노드의 부모 노드

        while(true) {
            parent = focusNode; //  이동

            if (key < parent.key) {             //  삽입하려는 키가 현재 노드보다 작으면
                focusNode = parent.leftChild;   //  왼쪽으로 이동

                if (focusNode == null) {        //  왼쪽 노드가 비어있으면
                    parent.leftChild = newNode; //  왼쪽 노드에 삽입
                    return;
                }
            } else {                            //  삽입하려는 키가 현재 노드와 같거나 크다면
                focusNode = parent.rightChild;  //  오른쪽으로 이동

                if (focusNode == null) {        //  오른쪽 노드가 비어있으면
                    parent.rightChild = newNode;//  오른쪽 노드에 삽입
                    return;
                }
            }
        }
    }
}
```



### 삭제

이진 탐색 트리에서 값을 삭제할 때에도 삭제할 값을 검색하는 과정을 거칩니다. 그리고 삭제할 값을 찾았을 때 3가지 경우를 고려해야 합니다.

1. 삭제할 노드가 leaf 노드인 경우

   이 경우에는 해당 노드를 삭제하면 됩니다.

   <p align="center"><img src="./img/BST_delete1.png" width="600"></p>


2. 삭제할 노드에 자식이 하나만 있는 경우

   이 경우에는 삭제할 노드의 자식 노드를 삭제할 노드의 부모 노드에 연결한 뒤 삭제하면 됩니다.

<p align="center"><img src="./img/BST_delete2.png" width="600"></p>


3. 삭제할 노드에 자식이 둘 있는 경우

   이 경우에는 이전보다 다소 복잡해집니다. 이 때는 삭제할 노드의 왼쪽 서브 트리에 있는 값 중 가장 큰 값, 또는 오른쪽 서브 트리에 있는 값 중 가장 작은 값 중 하나를 삭제할 노드의 부모 노드에 연결해야 합니다.

<p align="center"><img src="./img/BST_delete3.png" width="600"></p>

​		이렇게 하는 이유는 이진 탐색 트리의 규칙을 지킬 수 있는 최선의 방법이기 때문입니다.

​		루트의 왼쪽 서브트리의 가장 오른쪽 값은 루트보다 작은 가장 가까운 수이며, 오른쪽 서브트리		의 가장 왼쪽 값은 루트보다 큰 가장 가까운 수입니다.

<p align="center"><img src="./img/BST_delete3-2.png" width="600"></p>

​		50을 삭제하는 경우를 봅시다. 60은 루트 노드의 오른쪽 서브 트리의 가장 왼쪽 값입니다. 이 값		을 루트의 자리에 놓고 50을 60이 있던 자리에 넣은 뒤 자식이 없는 노드를 삭제할 때 처럼 삭제		하면 됩니다.

``` java
public boolean deleteNode(int key) {
    // focusNode 와 parent 가 같을 수 있는 경우는 찾으려는 key 가 root 인 경우
    Node focusNode = root;
    Node parent = root;

    boolean isLeftChild = true;

    // while 문이 끝나고 나면 focusNode 는 삭제될 노드를 가리키고, parent 는 삭제될 노드의 부모노드를 가리키게 되고, 삭제될 노드가 부모노드의 left 인지 right 인지에 대한 정보를 가지게 된다
    while(focusNode.key != key) {
        parent = focusNode; // 삭제할 노드를 찾는 과정중(while문)에서 focusNode 는 계속해서 바뀌고 parent 노드는 여기서 기억해둔다

        if(key < focusNode.key) {
            isLeftChild = true;             // 지우려는 노드가 왼쪽에 있는 노드냐 기록용
            focusNode = parent.leftChild;
        } else {
            isLeftChild = false;            // 지우려는 노드가 오른쪽에 있는 노드냐 기록용
            focusNode = parent.rightChild;
        }

        // 찾으려는 노드가 없는 경우
        if(focusNode == null) {
            return false;
        }
    }


    Node replacementNode;
    // 지우려는 노드의 자식 노드가 없는 경우
    if(focusNode.leftChild == null && focusNode.rightChild == null) {
        if (focusNode == root)
            root = null;
        else if (isLeftChild)
            parent.leftChild = null;
        else
            parent.rightChild = null;
    }
    // 지우려는 노드의 오른쪽 자식노드가 없는 경우 (왼쪽 자식 노드만 있는 경우)
    else if(focusNode.rightChild == null) {
        replacementNode = focusNode.leftChild;

        if (focusNode == root)
            root = replacementNode;
        else if (isLeftChild)
            parent.leftChild = replacementNode;
        else
            parent.rightChild = replacementNode;
    }
    // 지우려는 노드의 왼쪽 자식노드가 없는 경우 (오른쪽 자식 노드만 있는 경우)
    else if (focusNode.leftChild == null) {
        replacementNode = focusNode.rightChild;
        if (focusNode == root)
            root = replacementNode;
        else if (isLeftChild)
            parent.leftChild = replacementNode;
        else
            parent.rightChild = replacementNode;
    }
    // 지우려는 노드의 양쪽 자식노드가 모두 있는 경우
    // 오른쪽 자식 노드의 sub tree 에서 가장 작은 노드를 찾아서 지우려는 노드가 있던 자리에 위치시킨다
    else {
        // 삭제될 노드의 오른쪽 sub tree 를 저장해둔다
        Node rightSubTree = focusNode.rightChild;

        // 삭제될 노드 자리에 오게 될 새로운 노드 (오른쪽 sub tree 에서 가장 작은 값을 가진 노드)
        // 이 노드는 왼쪽 child 가 없어야 한다 (가장 작은 값이기 때문에)
        replacementNode = getRightMinNode(focusNode.rightChild);

        if (focusNode == root)
            root = replacementNode;
        else if (isLeftChild)
            parent.leftChild = replacementNode;
        else
            parent.rightChild = replacementNode;

        replacementNode.rightChild = rightSubTree;
        // 지우려는 노드의 오른쪽 sub tree 에 노드가 하나밖에 없는 경우
        if (replacementNode == rightSubTree) 
            replacementNode.rightChild = null;

        replacementNode.leftChild = focusNode.leftChild; // 지우려는 노드의 왼쪽 sub tree 를 연결시킨다
    }

    return true;
}

private Node getRightMinNode(Node rightChildRoot) {
    Node parent = rightChildRoot;
    Node focusNode = rightChildRoot;

    while (focusNode.leftChild != null) {
        parent = focusNode;
        focusNode = focusNode.leftChild;
    }

    parent.leftChild = null;
    return focusNode;
}
```



### 시간 복잡도

이진 탐색 트리의 삽입, 검색, 삭제에 대한 시간 복잡도는 **균형 상태이면 O(logN)**, **불균형 상태라면 최대 O(N)** 입니다.

<p align="center"><img src="./img/BST_time_complexity.png" width="600"></p>





## 이진 탐색트리가 중복된 데이터를 갖지 않는 이유

이진 탐색 트리는 목표 데이터 값의 빠른 탐색을 위한 자료구조입니다. 만약 이진 탐색 트리가 중복된 데이터 값을 가진다면 이진 탐색 트리에서 데이터를 탐색할 때 두가지 경우를 생각해야 할 것입니다.

1. 처음으로 발견되는 노드만 찾고, 중복 노드가 존재할 가능성을 무시한다.
   - 그렇다면 이진 탐색트리는 전혀 사용되지 않을 노드를 갖게 될 것입니다. 이는 메모리 낭비에 불과하며 잘못된 자료구조입니다.
2. 중복 노드가 존재할 가능성이 있으므로 중복 노드를 모두 탐색한다.
   - 그렇다면 일치하는 데이터를 찾았지만 중복 노드를 찾기 위해서 계속 탐색을 진행할 것입니다. 이는 시간의 낭비이며 빠른 탐색에 맞지 않는 자료구조입니다.



#### 관련 문제

[백준 5639 이진 검색 트리](https://www.acmicpc.net/problem/5639)

[백준 18240 이진 탐색 트리 복원하기](https://www.acmicpc.net/problem/18240)



# 우선순위 큐와 힙

## 우선순위 큐

일반적으로 큐(Queue)는 먼저 들어온 데이터가 먼저 나가는 선입선출(First in-First out) 구조입니다.  하지만, 우선순위 큐는 데이터가 들어온 순서가 아닌 데이터의 **우선 순위가 높은 데이터가 먼저 나가는 큐**를 말합니다. 우선순위 큐는 **주로 힙(Heap)이라는 자료구조로 구현**합니다. 또한 우선순위 큐는 **데이터의 우선 순위를 비교해야 하므로 그 데이터는 Comparable 하거나 comparator를 생성자에 넣어서 비교 연산이 가능하도록** 해야 합니다.



## 힙 

힙(Heap)은 완전 이진 트리에 있는 노드 중에서 키 값이 가장 큰 노드나 가장 작은 노드를 찾게 만든 자료구조입니다. 힙은 이진탐색트리와 달리 중복된 값이 허용됩니다.

<p align="center"><img src="./img/max_heap.png" width="600"></p>

- 최대 힙
  - 키 값이 가장 큰 노드를 찾기 위한 완전 이진 트리
  - 부모 노드의 키 값 >= 자식 노드의 키 값
  - 루트 노드:  키 값이 가장 큰 노드



<p align="center"><img src="./img/min_heap.png" width="500"></p>

- 최소 힙
  - 키 값이 가장 작은 노드를 찾기 위한 완전 이진 트리
  - 부모 노드의 키 값 < 자식 노드의 키 값
  - 루트 노드: 키 값이 가장 작은 노드
- 힙에서는 루트 노드의 원소만을 삭제 할 수 있습니다.
  - 루트 노드의 원소를 삭제하여 반환한다.
  - 루트 노드를 다시 구한다.



## 힙 구현

힙은 일반적으로 배열을 이용하여 구현합니다. 힙은 완전 이진 트리이므로 중간에 비어있는 요소가 없기 때문에 배열의 공간을 모두 사용하기 때문입니다.

<p align="center"><img src="./img/heap.png" width="600"></p>

**자식노드를 구하고 싶을 때**

- 왼쪽 자식노드 index = (부모 노드 index) * 2
- 오른쪽 자식노드 index = (부모 노드 index) * 2 + 1

**부모노드를 구하고 싶을 때**

- 부모 노드 index = (자식노드 index) / 2



### 삽입

힙에 데이터를 삽입하는 방법은 다음과 같습니다.

1. 완전 이진트리의 마지막 노드에 이어서 새로운 노드를 추가한다.
2. 추가된 새로운 노드를 부모의 노드와 비교하여 교환한다.
3. 힙 구조를 만족할 때 까지 2를 반복한다.

힙에 3을 삽입하는 과정은 다음과 같습니다.

<p align="center"><img src="./img/heap_insert.png" width="500"></p>

### 삭제

힙에서 데이터를 삭제하는 방법은 다음과 같습니다.

1. 루트 노드가 가장 우선 순위가 높으므로 루트 노드를 삭제한다.
2. 루트 노드가 삭제된 빈자리에 완전 이진트리의 마지막 노드를 가져온다.
3. 루트 자리에 위치한 새로운 노드를 자식 노드와 비교하여 교환한다.
4. 힙 구조를 만족할 때 까지 3을 반복한다.

<p align="center"><img src="./img/heap_delete.png" width="500"></p>



## 힙의 시간 복잡도와 우선순위 큐를 힙으로 구현하는 이유

우선순위 큐 == 힙이 아닙니다. 힙은 우선순위 큐를 구현할 수 있는 여러 자료구조 중 하나입니다. 하지만 우선순위 큐는 보통 힙으로 구현합니다. 그 이유는 시간 복잡도에서 가장 유리하기 때문입니다.

만약 배열로 구현한다면 우선 순위가 높은 순서대로 배열의 가장 앞부분부터 넣을 때 우선 순위가 높은 데이터는 맨 앞의 index를 이용하는 것으로 쉽게 찾을 수 있습니다. 하지만 우선 순위가 중간인 데이터를 삽입한다면 삽입하는 위치를 찾고, 뒤의 데이터를 모두 한 칸 씩 밀어야 합니다. 그러므로 정렬된 배열에서는 삽입은 O(n), 삭제는 O(1)의 시간 복잡도를 가집니다. 정렬되지 않은 배열이라면 삽입은 O(1)이고, 삭제할 때 삭제할 값을 찾아야 하므로 O(n)의 시간 복잡도를 가집니다.

연결리스트로 구현할 때도 배열과 크게 다르지 않습니다. 정렬 유무에 따라 배열과 같은 시간 복잡도를 가집니다.

하지만 힙으로 구현한다면  삽입과 삭제과정에서 모두 부모 노드와 자식 노드 간의 비교만 이뤄지므로 O(log2n)의 시간 복잡도를 가집니다. 

정리하면 배열과 연결리스트는 정렬 여부에 따라 삽입과 삭제에서 O(n)과 O(1)의 시간 복잡도를 가지게 되고, 힙은 일관적으로 O(log2n)의 시간복잡도를 가집니다. 그래서 편차가 심한 배열과 연결리스트 보다는 힙으로 구현을 합니다.



#### 관련 문제

[백준 11279 최대 힙](https://www.acmicpc.net/problem/11279)

[백준 1655 가운데를 말해요](https://www.acmicpc.net/problem/1655)

[백준 11000 강의실 배정](https://www.acmicpc.net/problem/11000)



# 트라이 Trie

키와 값을 쌍으로 갖는 연관 배열 데이터를 저장하는 트리 자료 구조

주로 자연어 처리(NLP) 분야에서 문자열 탐색을 위해 사용한다. 문자열의 각각의 문자 단위로 색인을 구축한 형태이다.

## 트라이 원리

### 트라이를 이용한 문자열 저장

![Trie Principle](https://github.com/hongcheol/CS-study/blob/main/Algorithm/img/trie-principle.png?raw=true)

<p align="center"><img src="https://github.com/hongcheol/CS-study/blob/main/Algorithm/img/trie-search.png?raw=true" alt="Trie Search" width="500"></p>

> 1. 각 노드가 배열로 구성된 트리를 생성한다.
> 2. 저장하려는 문자열의 모든 문자들을 확인하며 아래 과정을 시행한다.
> 3. 루트 노드(문자 배열)에서 문자열의 첫번째 문자에 해당하는 인덱스로 이동한다.
> 4. 해당 인덱스에 연결된 자식 노드가 존재하지 않는다면 새로운 노드(문자 배열)를 할당한다. 이후 새로운 노드에서 두번째 문자에 해당하는 인덱스로 이동한다.
> 5. 해당 인덱스에 연결된 자식 노드가 존재한다면 해당 노드의 두번째 문자에 해당하는 인덱스로 이동한다.
> 6. 문자열의 모든 문자를 다 저장할 때까지(즉, 문자열 길이만큼) 위의 과정을 반복한다. 마지막 문자를 저장하고 나서는 배열 값을 `true`로 설정하여 하나의 문자열이 완전히 저장됨을 표시한다.

위의 과정에서 주목할 점은 **접두사가 동일한 문자열**을 저장할 경우 **최소 하나 이상의 노드를 공유**한다는 것이다!

## 트라이의 시간 복잡도

위의 방식대로 문자열을 저장할 경우 **한 문자열을 탐색할 때 고작 `O(1)`의 연산**만 필요하게 된다.

아무리 트리 노드가 많이 존재하더라도, 심지어는 전세계 모든 인구인 80억명의 이름이 저장된 트라이라고 할지라도, 오직 `O(1)`의 시간만이 걸린다. 그 이유는 한 문자열을 탐색하기 위해서는 해당 문자열이 갖고 있는 문자 노드만을 탐색하기 때문이다. 루트 노드에서부터 **해당 문자열의 길이** 만큼의 자식 노드만 타고 들어가기 때문에, 다른 탐색 알고리즘과 달리 **저장된 문자열의 개수에 영향을 받지 않는다**는 점이 트라이의 큰 특장점이다.

## 트라이의 공간 복잡도

우수한 시간적 성능을 자랑하는 트라이의 치명적인 한계는 바로 메모리를 많이 사용한다는 것이다.

트라이를 이용해 한국의 5천만 인구의 이름을 저장한다고 생각해보자. 한국에서 가장 긴 이름은 `"박하늘별님구름햇님보다사랑스러우리"`로 17자이다. 이 이름을 저장하기 위해서는 한글의 모든 음절을 배열로 담은, 길이가 `11,172인 배열`이 `17개`가 필요하고 총 `189,924`만큼의 메모리(1음절을 1byte로 가정)가 필요하다. 이름의 길이가 길어서 생긴 문제라고 말할 수도 있겠지만, 가장 보편적인 3자 이름 역시 `11,172인 배열`이 `3개`가 필요하고 총 `33,516`만큼의 메모리가 필요하다. 한 명의 이름을 저장하는데도 이렇게 많은 메모리를 사용하는데 이런 방식으로 `5천만`명의 이름을 저장한다면 어마어마한 크기의 메모리가 필요하게 될 것이다.(물론 같은 성을 사용하거나 돌림자를 사용하는 이름은 노드를 같이 사용할 수 있어 조금은 효율적이라고 말할 수 있다.)

트라이의 공간 복잡도는 대략 `O(포인터 크기 * 포인터 배열의 길이 * 전체 노드 개수)`가 된다. 따라서 트라이는 **시간적 성능과 공간적 성능을 맞바꾼** 대표적인 예로 볼 수 있다.

## Java를 이용한 트라이 구현

```java
class Trie {
    final int ALPHABET_SIZE = 26;         // 포인터 배열의 길이(표현할 수 있는 문자 개수)

    class Node {
        Node[] children = new Node[ALPHABET_SIZE];     
        boolean isEndOfWord;              // 저장하려는 문자열의 마지막 문자 여부

        public Node() {
            for (int i=0; i<ALPHABET_SIZE; i++) {
                children[i] = null;       // 포인터 배열 초기화
            }

            isEndOfWord = false;          // 마지막 문자 여부 초기화
        }
    }

    Node root;                            // 문자열의 첫번째 문자

    public void insert(String key) {
        int length = key.length();        // 탐색하려는 문자열 길이
        int alphabetIdx;                  // 문자열의 각 문자의 인덱스
        Node curAlphabet = root;          // 현재 탐색중인 문자열의 문자

        for (int level=0; level<length; level++) {
            alphabetIdx = key.charAt(level) - 'a';                    // 문자열의 각 문자의 인덱스 구하기
            Node nextAlphabet = curAlphabet.children[alphabetIdx];    // 문자열의 다음 문자

            if (nextAlphabet == null) {         // 찾으려는 문자에 연결된 자식 노드가 없을 경우
                nextAlphabet = new Node();      // 새로운 노드 생성 뒤 자식 노드로 연결
            }

            curAlphabet = nextAlphabet;         // 찾으려는 문자의 자식 노드를 현재 노드로
        }

        curAlphabet.isEndOfWord = true;         // 문자열의 모든 문자에 대해 삽입이 끝났다면 마지막 문자의 노드는 true로 변경하여 하나의 문자열이 저장됐음을 표시
    }

    public boolean search(String key) {
        int length = key.length();        // 탐색하려는 문자열 길이
        int alphabetIdx;                  // 문자열의 각 문자의 인덱스
        Node curAlphabet = root;          // 현재 탐색중인 문자열의 문자

        for (int level=0; level<length; level++) {
            alphabetIdx = key.charAt(level) - 'a';
            Node nextAlphabet = curAlphabet.children[alphabetIdx];

            if (nextAlphabet == null) {         // 탐색하려는 문자열이 존재하지 않는 경우
                return false;
            }

            curAlphabet = nextAlphabet;         // 찾으려는 문자의 자식 노드를 현재 노드로
        }

        return (curAlphabet.isEndOfWord);       // 탐색하려는 문자열이 존재하는 경우
    }
}
```

## PS 문제 추천

[2020 KAKAO BLIND RECRUITMENT > 가사검색](https://programmers.co.kr/learn/courses/30/lessons/60060)

<br>



# 알고리즘 설계 패러다임
# 분할 정복
## 분할 정복(Divide and Conquer)이란?
한 문제를 둘 이상의 **부분 문제(sub-problem)** 로 나누어 해결하고 이를 합쳐 원래 문제를 해결하는 기법

분할 정복 알고리즘은 다음과 같이 세 부분으로 나누어서 생각해볼 수 있다.

1. **분할(Divide)** : 원래 문제를 분할하여 더 작은 하위 문제들 나눈다.

2. **정복(Conquer)** : 하위 문제 각각을 재귀적으로 해결

3. **병합(merge)** : 하위 문제들의 답을 합쳐서 원래 문제를 해결
 
<br>

분할 정복을 적용하기 위해서는 문제에 다음과 같은 몇 가지 특성이 성립해야 한다.

1. 부분 문제로 나누는 자연스러운 방법이 있어야 한다.

2. 부분 문제의 답을 조합해 원래 문제의 답을 계산하는 효율적인 방법이 있어야 한다.

   *(분할 정복을 사용한다고 무작정 효율이 좋아지는 것은 아니다.)*

<br>

## 분할 정복의 장/단점
- 장점 👍
  - 문제를 나눔으로써 어려운 문제를 해결할 수 있다는 장점이 있다. 그리고 이 방식이 그대로 사용되는 효율적인 알고리즘들도 여럿 있으며, 문제를 나누어 해결한다는 특징상 병렬적으로 문제를 해결하는 데 큰 강점이 있다.
  - 보통, 분할 정복의 경우 작은 문제로 분할함으로써 같은 작업을 더 빠르게 처리할 수 있게 해준다. (수행 시간 감소)

- 단점 👎
  - 함수를 재귀적으로 호출한다는 점에서 함수 호출로 인한 오버헤드가 발생하며, 스택에 다양한 데이터를 보관하고 있어야 하므로 스택 오버플로우가 발생하거나 과도한 메모리 사용을 하게 되는 단점이 있다.

<br>

## 일반적인 재귀 호출과 다른 점 

<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131237135-fd55bdac-c852-4681-b3f3-a6c4093fff28.png" width="50%">
</p>

* 분할 정복이 일반적인 재귀 호출과 다른 점은 **문제를 한 조각과 전체를 나누는 대신 거의 같은 크기의 부분 문제로 나누는 것** 이다.

* 보통 재귀 함수를 사용해서 분할 정복 알고리즘을 구현하지만, 분할 정복이라고 해서 반드시 재귀 함수를 이용하는 것은 아니다.

   함수 호출시 발생하는 오버헤드를 없애기 위해서 스택이나 큐 등을 이용하는 경우도 있다.

<br>

## 분할 정복 알고리즘 활용 예시

분할 정복이 쓰이는 예는 **이분검색, 병합정렬, 퀵정렬, 최대값 찾기, 임계값의 결정, 쉬트라센 행렬곱셈 알고리즘** 등이 있다.

<br>

### 병합 정렬과 퀵정렬 (같은 문제를 어느 단계에서 해결하느냐에 따른 구분)

병합 정렬(merge sort)과 퀵 정렬(quick sort)은 분할 정복 패러다임을 기반으로 해서 만들어진 대표적인 정렬 알고리즘이다.

이 두 알고리즘은 같은 아이디어로 정렬을 수행하지만 시간이 많이 걸리는 작업을 **분할 단계**에서 하느냐, **병합 단계**에서 하느냐가 다르다.

이렇게 같은 문제를 해결하는 알고리즘이더라도 어떤 식으로(어느 단계에서) 분할하느냐에 따라 다른 알고리즘이 될 수 있다.

<br>

### 병합 정렬

<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131238392-d6591d56-4690-48b0-83c9-7e027e2fcda4.png" width="40%">
</p>

* 전체 수행 시간은 **병합 과정**에 의해 지배된다.

* **O(n)** 시간이 걸리는 과정을 **재귀 호출 후에 진행** (병합 과정)

   문제의 수는 항상 절반으로 나눠지기 때문에 필요한 단계 수는 **O(logn)**

* 시간 복잡도 : 항상 **O(nlogn)** 으로 일정

<br>

### 퀵 정렬
<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131238476-bd4f5db7-48f1-41f2-bca7-f888cb02f133.png" height="200"> <img src="https://user-images.githubusercontent.com/33649908/131238219-42fd1206-d152-4924-bfa2-0f53e122fe58.png" height="200">
</p>

* 전체 수행 시간은 두개 부분 문제로 나누는 **파티션(partition) 과정**에 의해 지배된다. 분할된 두 부분 문제가 비슷한 크기로 비슷한 크기로 나눠진다는 보장이 없기 때문에, 이를 비슷한 크기로 나누는 좋은 기준을 선택하는 것은 퀵정렬에서 중요한 요소이다.

* **O(n)** 시간이 걸리는 과정을 **재귀 호출 전**에 진행 (분할)

   문제의 수가 항상 절반으로 나누어 진다는 보장이 없기 때문에 필요한 단계수를 정확히 계산하기 힘들다.
   
   **최악의 경우 n, 평균적인 경우 logn**만큼의 단계가 필요하다.

* 시간 복잡도 : 최악 = **O(n^2)**, 평균 = **O(nlogn)**

<br>

### 관련 문제
[백준 1629번 곱셈](https://www.acmicpc.net/problem/1629)

[백준 10830번 행렬 제곱](https://www.acmicpc.net/problem/10830)

<br>

# 동적 계획법
## 동적 계획법(Dynamic Programming, DP)이란?

동적 계획법은 주어진 문제를 풀기 위해서, 문제를 여러 개의 **하위 문제(subproblem)** 로 나누어 푼 다음, 그것을 결합하여 해결하는 방식이다.

<br>

<p align="center">
<img src="https://user-images.githubusercontent.com/33649908/131238559-1a8c7588-3713-475d-b740-69629b9b9fb4.png" width="50%">
</p>

동적 계획법은 처음 주어진 문제를 더 작은 문제들로 나눈 뒤 각 조각의 답을 계산하고, 이 답들로부터 원래 문제에 대한 답을 계산해 낸다는 점에서 분할 정복(Divide and Conquer)과 비슷하다. 하지만 가장 큰 차이점은 **동적 계획법에서는 쪼개진 작은 문제가 중복되지만, 분할 정복은 절대로 중복될수가 없다는 점**이다.

다시 말하면, 동적 계획법과 분할 정복의 차이는 **문제를 나누는 방식**이다. 동적 계획법에서는 어떤 부분 문제는 두 개 이상의 문제를 푸는데 사용될 수 있기 때문에, 이 문제의 답을 여러 번 계산하는 대신 **한 번만 계산하고 그 결과를 재활용함으로써 속도를 향상**시킬 수 있다. 이때 이미 계산한 값을 저장해 두는 메모리를 캐시(cache)라고 부르며, 두 번 이상 계산되는 부분 문제를 중복되는 **부분 문제(overlapping subproblems)** 라고 부른다.

<br>

* **동적 계획법의 조건**

   두 가지 속성을 만족해야 동적 계획법으로 문제를 풀 수 있다.

1. **Overlapping Subproblem**
   : 중복되는 부분 문제(overlapping subproblem) 는 어떤 문제가 여러 개의 부분 문제(subproblem)으로 쪼개질 수 있을 때 사용하는 용어이다. 이때 '부분 문제'란, 항상 새로운 부분 문제를 생성해내기 보다는 계속해서 같은 부분 문제가 여러 번 재사용되거나 재귀 알고리즘을 통해 해결되는 문제를 가리킨다.

2. **Optimal Substructure**
   : 최적 부분구조(optimal substructure)는 어떤 문제의 최적의 해결책이 그 부분 문제의 최적의 해결책으로 부터 설계될 수 있는 경우를 말한다. 즉, 최적 부분구조 일때 문제의 정답을 작은 문제의 정답에서부터 구할 수 있다. 이 속성은 동적 계획법이나 그리디 알고리즘의 유용성을 판별하는데 사용되기도 한다.
   
<br>

* **메모리제이션(Memorization)**

메모이제이션은 컴퓨터 프로그램이 동일한 계산을 반복해야 할 때, **이전에 계산한 값을 메모리에 저장함**으로써 동일한 계산의 반복 수행을 제거하여 **프로그램 실행 속도를 빠르게 하는 기술**이다. 동적 계획법의 핵심이 되는 기술이다.

동적 계획법에서 각 문제는 한 번만 풀어야 한다. (중복되는 부분 문제를 여러번 풀지 않는다는 뜻) Optimal Substructure를 만족하기 때문에 같은 문제는 구할 때마다 정답이 같다. 따라서 정답을 한 번 구했으면 그 정답을 캐시에 메모해놓는다. 이렇게 메모하는 것을 코드의 구현에서는 배열에 저장하는 것으로 할 수 있다. 이를 메모리제이션이라고 한다.

<br>

## 동적 계획법의 장/단점

- 장점 👍
  - 필요한 모든 가능성을 고려해서 구현하므로 항상 최적의 결과를 얻을 수 있다.
  
  - 메모리에 저장된 값을 사용하므로 큰 문제를 빠른 속도로 해결하여 최적의 해를 찾아낼 수 있다.

- 단점 👎
  - 모든 가능성에 대한 고려가 불충분할 경우 최적의 결과를 보장할 수 없다.
  
  - 다른 방법론에 비해 많은 메모리 공간을 요구한다.
  
<br>

## 동적 계획법의 구현 방법

동적 계획법의 구현 방식에는 두 가지 방법이 있다.

1. **Top-down** : 큰 문제를 작은 문제로 쪼개면서 푼다. **재귀**로 구현
2. **Bottom-up** : 작은 문제부터 차례대로 푼다. **반복문**으로 구현

Top-down과 Botton-up의 시간복잡도 차이는 문제에 따라 다를 수 있으므로 정확히 알 수는 없다. Top-down은 재귀 호출을 하기때문에 스택의 사용으로 시간이 더 걸릴 것이라고 생각할 수 있겠지만, 실제로 그 차이는 크지 않다. 

(다만, 파이썬의 경우 재귀 호출 시 스택 오버 플로우(stack overflow)가 발생할 수 있기 때문에, Bottom-up으로 구현하는 것이 좋다. C++과 Java에서는 재귀로 구현하는 것이 크게 문제가 되지 않는다.)

💡 Top-down으로만 해결가능하거나 Bottom-up으로만 해결가능한 문제는 극히 드문 경우이므로, 아무거나 선택해서 사용하면 된다.

<br>

## 동적 계획법의 활용 예시

동적 계획법의 예시로는 피보나치 수열 구하기, 이항계수 구하기, 최단경로의 플로이드 알고리즘, 최적화 문제, 외판원 문제 등이 있다.

<br>

### 관련 문제
[백준 2294번 동전2](https://www.acmicpc.net/problem/2294)

[백준 1463번 1로 만들기](https://www.acmicpc.net/problem/1463)

----

# 그래프
# 그래프의 표현과 정의
어떤 자료나 개념을 표현하는 정점(vertex)들의 집합 V와 이들을 연결하는 간선(edge)들의 집합 E로 구성된 자료구조

주로 현실 세계의 사물이나 추상적인 개념 간의 연결관계를 표현할 때 사용

## 그래프 관련 용어

- 노드 (Node): 위치를 말함, 정점(Vertex)라고도 함

- 간선 (Edge): 위치 간의 관계를 표시한 선으로 노드를 연결한 선이라고 보면 됨 (link 또는 branch 라고도 함)
- 인접 정점 (Adjacent Vertex) : 간선으로 직접 연결된 정점(또는 노드)
- 참고
  - 정점의 차수 (Degree): 무방향 그래프에서 하나의 정점에 인접한 정점의 수
  - 진입 차수 (In-Degree): 방향 그래프에서 외부에서 오는 간선의 수
  - 진출 차수 (Out-Degree): 방향 그래프에서 외부로 향하는 간선의 수
  - 경로 길이 (Path Length): 경로를 구성하기 위해 사용된 간선의 수
  - 단순 경로 (Simple Path): 처음 정점과 끝 정점을 제외하고 중복된 정점이 없는 경로
  - 사이클 (Cycle): 단순 경로의 시작 정점과 종료 정점이 동일한 경우

## 그래프의 종류

<img width="731" alt="그래프_종류" src="https://user-images.githubusercontent.com/16794320/127731827-67d5dabf-871b-4b8c-a150-07b1749593a8.png">

### 방향 그래프

그래프의 각 간선이 방향이라는 속성을 갖는 그래프

### 가중치 그래프

그래프의 각 간선이 가중치(weight)라는 송석을 갖는 그래프

### 다중 그래프

두 정점 사이에 두 개 이상의 간선이 있을 수 있는 그래프

### 트리

간선을 통해 두 정점을 잇는 방법이 딱 하나밖에 없는 그래프

### 이분그래프

그래프의 정점들을 겹치지 않는 두 개의 그룹으로 나눠서 서로 다른 그룹에 속한 정점들 사이에만 간선이 존재하도록 만들 수 있는 그래프

### DAG

사이클 없는 방향 그래프(Directed Acyclic Graph)

기본적으로 방향 그래프, 한 점에서 출발해 자기 자신으로 돌아오는 경로가 없는 경우

## 그래프의 경로

그래프에서 경로란 끝과 끝이 연결된 간선들을 순서대로 나열한 것
<img width="303" alt="그래프_경로" src="https://user-images.githubusercontent.com/16794320/127731829-9ac04786-3ff6-4711-b67f-0e3f7f12233c.png">

주어진 그림에서 1에서 5로 가는 경로는 
(1,2),(2,4),(4,5)와 같이 표현.
간단하게 1-2-4-5로도 표현

## 그래프의 표현 방법

V = 정점의 수

### 인접 리스트

그래프의 각 정점마다 해당 정점에서 나가는 간선의 목록을 저장해서 그래프를 표현

각 정점마다 하나의 연결 리스트를 갖는 방식으로 구현

### 인접 행렬

인접 리스트가 두 정점의 연결 관계를 확인하기위해 모든 리스트를 뒤져야한다는 단점 보완

|V|X|V| 크기의 행렬(|V|는 정점의 갯수)로 표현한다.

간**선의 수가 $V^2$에 비해서 훨씬 적은 경우 인접리스트를 사용하는 것이 유리하고**

**간선의 수가 $V^2$에 비례하는 경우 인접행렬을 사용하는 것이 유리하다.**

### 암시적 그래프 표현

그래프를 직접 메모리에 표현하지않고 그래프 구조만 사용하는 것이 유리한 경우

- 입력이 그래프의 형태를 띄지않는 문제의 경우(ex. 배열로 주어진 미로)
- 그래프의 크기가 아주 큰데 실제 사용하는 부분은 그래프의 일부분인 경우

# DFS

DFS(Depth-First Search,깊이 우선 탐색)은 그래프의 모든 노드를 탐색하는 가장 단순한 방법입니다.

정점의 자식들을 먼저 탐색하는 방식으로 다음의 순서를 따릅니다.

1. 현재 정점과 인접한 간선들을 하나씩 검사한다.
2. 아직 방문하지 않은 정점으로 향하는 간선이 있다면 그 간선을 따라간다.
3. 더 이상 갈 곳이 없는 막힌 정점에 도달할 때까지 반복한다.
4. 더이상 갈 곳이 없다면 가장 마지막에 지난 간선을 따라 돌아가 더 이상 방문할 정점이 없을 때까지 반복한다.

각 정점이 정수형인 경우를 예시로 설명하겠습니다.


![그래프_표](./img/graph_chart.png)

위와 같이 만들어진 그래프를 DFS로 탐색하는 그림은 다음과 같습니다.

<img src="./img/graph_DFS.png" alt="그래프_DFS" style="zoom:50%;" />


## Java로 그래프를 표현하는 방법

정점의 개수를 n, 간선의 개수를 m,  연결관계에 있는 노드를 (node1, node2)의 순서쌍으로 하면, 다음과 같이 그래프를 표현할 수 있습니다.

```java
Map<Integer, ArrayList<Integer>> graph = new TreeMap<Integer, ArrayList<Integer>>();
int n = 0, m = 0
Scanner sc = new Scanner(System.in);
n = sc.nextInt();
m = sc.nextInt();
//초기화 해줘야지 아래의 반복문에서 nullPointException 발생하지않는다.
for(int i = 0;i<n;i++){
  graph.put(i+1,new ArrayList<>());
}
for(int i = 0;i<m;i++){
  int n1 = 0, v1 = 0;
  node1 = sc.nextInt();
  node2 = sc.nextInt();
  graph.get(n1).add(node2);
  graph.get(node2).add(node1);
}
```



## DFS 알고리즘 구현

스택을 활용해서 구현할 수 있습니다.

```java
//code
public void dfsWithoutRecursion(int start) {
  Stack<Integer> stack = new Stack<Integer>();
  boolean[] isVisited = new boolean[adjVertices.size()];
  stack.push(start);
  while (!stack.isEmpty()) {
    int current = stack.pop();
    isVisited[current] = true;
    visit(current);
    for (int dest : adjVertices.get(current)) {
      if (!isVisited[dest])
        stack.push(dest);
    }
  }
}
```

재귀호출을 통해 메서드 스택을 이용해서 구현하는 방법도 있습니다.

```java
public void dfs(int start) {
  boolean[] isVisited = new boolean[adjVertices.size()];
  dfsRecursive(start, isVisited);
}
void dfsRecursive(int current, boolean[] isVisited) {
  isVisited[current] = true;
  visit(current);
  for (int dest : adjVertices.get(current)) {
    if (!isVisited[dest])
      dfsRecursive(dest, isVisited);
  }
}
```



## 시간 복잡도

일반적으로 DFS의 시간복잡도는 정점의 수를 V, 간선의 수를 E라고 할 때 O(V+E) 입니다.

# BFS

BFS(너비 우선 탐색)은 그래프를 탐색하는 방법 중 하나입니다.

정점들과 같은 레벨에 있는 노드(형제 노드)들을 먼저 탐색하는 방법으로 다음의 순서를 따릅니다. 

1. 현재 정점과 인접한 간선들을 하나씩 검사합니다.

2. 현재 노드에서 방문할 수 있는 노드를 전부 방문합니다.

3. 전부 방문한 후 그 다음 레벨의 노드를 방문합니다.

4. 더 이상 방문할 곳이 없다면 탐색을 종료합니다.

   

   ![그래프_표](./img/graph_chart.png)

<img src="./img/graph_BFS.png" alt="graph_BFS" style="zoom:50%;" />

## BFS 알고리즘 구현

queue와 배열을 이용해서 구현할 수 있습니다.

```java
//code
static void bfs(Map graph,int start_node){
  Queue<Integer> need_visit = new LinkedList<>();
  Queue<Integer> visited = new LinkedList<>();
  need_visit.add(start_node);

  while(need_visit.isEmpty()==false){
    int node = need_visit.poll();
    if(!visited.contains(node)){
      visited.add(node);
      ArrayList<Integer> temp = (ArrayList<Integer>) graph.get(node);
      for(int data : temp) need_visit.add(data);
    }
  }
  for(int data : visited)System.out.print(data+" ");
}
```



## 시간복잡도

일반적으로 BFS의 시간복잡도는 정점의 수를 V, 간선의 수를 E라고 할 때 O(V+E) 입니다.

# 최단 경로 알고리즘

### 최단 경로 문제란 무엇일까요?

최단 경로 문제란 두 노드를 잇난 가장 짧은 경로를 찾는 문제입니다. 즉, 가중치 그래프에서 간선의 가중치의 합이 최소가 되도록하는 경로를 찾는 것입니다.

그렇다면 최단 경로 문제는 어떤 종류가 있을까요??

1. 단일 출발 및 단일 도착 최단 경로 문제
2. 단일 출발 최단 경로 문제
3. 전체 쌍 최단 경로 문제

## 다익스트라

다익스트라 알고리즘은 최단 경로의 문제 종류 중 단일 출발 최단 경로 문제에 속합니다. 

그 이유는 다익스트라 알고리즘이 하나의 정점에서 다른 모든 정점 간의 가장 짧은 거리를 구하는 알고리즘이기 때문입니다.

### Basic idea

```java
//init
S = {v0}; distance[v0] = 0; 
for (w 가 V - S에 속한다면)//V-S는 전체 정점에서 이미 방문한 정점을 뺀 차집합.
	if ((v0, w)가 E에 속한다면) distance[w] = cost(v0,w); 
	else distance[w] = inf;

//algorithm
S = {v0};
while V–S != empty {
  //아래의 코드는 지금까지 최단 경로가 구해지지않은 정점 중 가장 가까운 거리를 반복한다는 의미
	u = min{distance[w], w 는 V-S의 원소};//---1
	
  S = S U {u}; 
  V–S = (V–S)–{u};
	for(vertex w : V–S)
		distance[w] = min{distance[w],distance[u]+cost(u,w)}//update distance[w];---2
}
```

위의 코드에서 1로 마킹한 부분에대한 설명을 하겠습니다.

먼저 length[v]는 최단 경로의 길이를 의미하고 distance[w]는 시작점에서부터의 최단 경로의 길이를 의미합니다.

S는 시작점을 포함해서 현재까지 최단 경로를 발견한 점들의 집합을 의미합니다.

<img src="./img/Dijkstra_0.png" alt="Dijkstra_0" style="zoom:75%;" />

u = min{distance[w], w 는 V-S의 원소}의 의미는 u까지의 최단 거리의 길이는 distance[u] 와 같다는 의미입니다. 

둘이 같다는 것은 다음처럼 증명할 수 있습니다.

![Dijkstra_1](./img/Dijkstra_1.png)

1. distance[u] > length[u]라고 가정해보겠습니다.
2. P를 시작점부터 u까지의 최단 경로라고 하면
3. P의 길이 = length[u]입니다.
4. P는 S에 속하지않는 최소 1개의 정점을 가집니다. 그렇지않다면 P의 길이 = distance[u]가 됩니다.
5. P의 경로에 속하면서 S에 속하지않는 시작점에서 가장 가까운 점을 w라 하겠습니다.
6. 그렇게 된다면 distance[u] > length[u] >= length[w] 가 성립하고, length[w] = distance[w]이기 때문에  이로부터
   **distance[u] > distance[w]**를 유추해낼 수 있습니다.
7. 이는 다익스트라 알고리즘은 아직 최단경로가 찾아지지않은 정점들만 선택한다는 정의에 어긋납니다.(지금 보는 점보다 가까운 경로가 존재한다면 이전 탐색에서 걸러져서 S에 포함되어있기 때문입니다.)

그렇기 때문에 distance[u] = length[u]입니다.

다음으로는 distance[w]를 업데이트하는 부분입니다.

<img src="./img/Dijkstra_2.png" alt="Dijkstra_2" style="zoom:50%;" />

위의 그림에서와 같이 시작점 v0에서 u를 거쳐 w로 가는 경로와 v0에서 w로 가는 경로의 길이 중 가까운 경로를 distance[w]로 설정하고 S와 V-S를 최신화해줍니다.

distance[ ]를 업데이트할 때 각각의 간선들이 2번씩 체크되는데 그 이유는 다음과 같습니다.

1. 

<img src="./img/Dijkstra_3.png" alt="Dijkstra_3" style="zoom:75%;" />

아직 최단 경로를 찾지 못한 정점 중 u에 인접한 정점들의 거리를 업데이트 할 때 u와 w를 연결하는 간선이 체크가 됩니다. 

2.

<img src="./img/Dijkstra_4.png" alt="Dijkstra_4" style="zoom:75%;" />

정점 w의 최단 경로가 찾아졌기 때문에, w는 S에 속하게됩니다. 아직 최단 경로를 찾기 못한 정점들에서 w까지의 거리를 업데이트하는 과정에서 u에서 w로의 간선이 다시 한 번 체크됩니다.



이런 방식으로 코드를 작성하면 각 정접들에대해서 u에 인접한 모든 간선을 살펴보는 연산이 추가되기 때문에 최악의 경우 시간복잡도가 O(𝑛^2)가 되버립니다. 이를 개선하는 방법으로는 대표적으로 우선순위 큐를 사용하는 방법(O( 𝑛 + 𝑚 log 𝑛))과 피보나치 힙을 사용하는 방법(O(𝑛 log 𝑛 + 𝑚))이 있습니다. 

피보나치 힙에대한 내용은 [피보나치 힙](https://en.wikipedia.org/wiki/Fibonacci_heap) 을 확인해주시고, 지금은 우선순위 큐에대한 내용을 다루겠습니다.

### 다익스트라 알고리즘 로직(우선순위 큐)

- 첫 번째 정점을 기준으로 연결되어있는 정점들을 추가해가면서 최단 거리를 갱신합니다.

  첫 정점부터 각 노드 사이의 거리를 저장하는 배열을 만든 후에 첫 정점의 인접 노드 간의 거리부터 먼저 계산하면서, 첫 정점부터 해당 노드 사이의 가장 짧은 거리를 해당 배열에 업데이트 합니다. 이런 로직은 현재의 정점에서 갈 수 있는 정점들부터 처리한다는 점에서 BFS와 비슷합니다. 

  다양한 다익스트라 알고리즘이 있지만 가장 개선된 형태인 우선순위 큐를 사용하는 방식을 다뤄보겠습니다.

  먼저 우선 순위 큐를 간단하게 설명하면 MinHeap 방식을 사용해서 현재 가장 짧은 거리를 가진 노드 정보를 먼저 꺼냅니다.

  꺼낸 노드는 다음의 과정을 반복합니다.

  1. 첫 정점을 기준으로 배열을 선언핸 첫 정점에서 각 정점까지의 거리를 저장합니다.
     - 초기에는 첫 정점의 거리를 0, 나머지는 무한대(inf)로 저장합니다.
     - 우선순위 큐에 순서쌍 (첫 정점, 거리 0) 만 먼저 넣어줍니다. 
  2. 우선순위 큐에서 노드를 꺼냅니다.
     - 처음에는 첫 정점만 저장된 상태이기때문에 첫번째 정점만 꺼내집니다.
     - 첫 정점에 인접한 노드들 각각에대해서 첫 정점에서 각 노드로 가는 거리와 현재 배열에 저장되어있는 첫 정점에서 각 정점까지의 거리를 비교합니다.
     - 배열에 저장되어 있는 거리보다, 첫 정점에서 해당 노드로 가는 거리가 더 짧은 경우, 배열에 해당 노드의 거리를 업데이트 합니다.
     - 배열에 해당 노드의 거리가 업데이트된 경우, 우선순위 큐에 해당 노드를 넣어줍니다.
  3. 2번의 과정을 우선순위 큐에서 꺼낼 노드가 없을 때까지 반복합니다.

- 우선순위 큐를 사용하면 지금까지 발견된 가장 짧은 거리의 노드에대해서 먼저 계산을 해서 더 긴 거리로 계산된 루트에 대해서는 계산을 스킵할 수 있다는 장점이 있습니다.

pseudo 코드는 다음과 같습니다. 

```java
found[], distance[]를 초기화
construct min_heap(V-{s});
for(i = 0; i < n-2; i++) { //𝑛−1iterations(=Θ(𝑛))---(1) 
  distance[u]가 최소인 정점 u를 선택합니다. //Θ(1) found[u] = T;로 바로 배열로 접근 가능
  min_heap에서 정점 u를 제거; //O(log𝑛)---(2)
  for(every vertex w adjacent to u) //Θ(𝑚)total---(a)
      if(found[w] == F && distance[u] + cost(u,w) < distance[w]){
        distance[w] = distance[u] + cost(u,w);
        adjust heap(w); //𝑂(log𝑛)foreachedgecheck---(b) 
  } // distance[w]가 수정됐기 때문에 heap을 조정해주는 for문
}
```

### 시간 복잡도

위의 pseudo code에서 다음 2가지 과정을 거칩니다.

(1),(a) - 각 정점마다 인접한 간선들을 모두 검사하는 과정 -> 𝑂(𝑛)

(2),(b) - 우선순위 큐에 정점/거리 정보를 넣고 삭제하는 과정 -> 𝑂(log𝑛)

따라서 전체 알고리즘은 O((𝑛+𝑚)log𝑛)입니다.

## 벨만-포드
다익스트라 알고리즘이 한 시작점에서 다른 모든 정점까지의 최단 거리를 구하는 유용한 알고리즘이지만, 음수 간선이 있는 그래프의 경우에는 그 정당성이 보장되지 않습니다. 벨만-포드 알고리즘은 이런 문제점을 해결하는 알고리즘입니다.
벨만-포드 알고리즘은 다익스트라 알고리즘과 똑같은 단일 시작점 최단 경로 알고리즘이지만, 음수 간선이 있는 그래프에 대해서도 최단 경로를 찾을 수 있습니다. 또한 그래프에 음수 사이클이 있어서 최단 거리가 제대로 정의 되지않을 경우도 알려줍니다.

벨만-포드 알고리즘은 시작점에서 각 정점까지 가는 최단 거리의 상한선을 적당하게 예측한 뒤에 예측 값과 실제 최단 거리 사이의 오차를 반복적으로 줄여가는 방식으로 동작합니다.
벨만-포드 알고리즘은 너비 우선 탐색을 기반으로 작동합니다.
수행 과정에서 각 정점까지의 최단 거리의 상한을 담은 배열 upper[ ]을 유지합니다.
이 값은 알고리즘이 진행되면서 점점 줄어들며, 알고리즘이 종료되는 시점에는 실제 최단 거리를 담게 됩니다.
### 벨만-포드의 동작 과정
1. 알고리즘이 시작되는 시점에는 그래프의 구조에 대해서 아는 것은 시작점에서 시작점까지의 최단 거리가 0이라는 것 뿐입니다. 그렇기 때문에 upper[s] = 0으로 초기화하고, 나머지 원소들은 모두 아주 큰 수인 INF = Integer.MAX_VALUE 와 같이 초기화를 합니다.
2. 벨만-포드 알고리즘은 이 예측값을 실제 최단 거리에 더 가깝게 갱신하기 위해서 다음과 같은 최단 거리의 특성을 이용합니다.
> 시작점에서 u와 v까지의 최단 거리 dist[u]와 dist[v]라 할 때 다음 조건은 항상 참입니다. w(u,v)는 u에서 v까지의 거리를 의미합니다.
> dist[v] <= dist[u] + w(u,v)

이 속성을 이용하면 upper의 값을 실제 최단 거리에 가깝게 보정할 수 있습니다.
upper[u] + w(u,v) < upper[v]인 상황을 통해서 예를 들어보겠습니다.
u까지 가는 최단 거리는 항상 upper[u]이거나 upper[u]보다 짧습니다. 그 뒤에 (u,v)를 붙인 경로의 길이는 최대 upper[u]+w(u,v)이기 때문에, upper[v]를 upper[u]+w(u,v)로 줄이는 것이 가능합니다.

3. 벨만-포드 알고리즘은 위와 같은 과정을 모든 간선에 대해서 반복적으로 실행하면서, 최종적으로 실제 최단 거리를 구할 수 있게됩니다.
### 벨만-포드의 종료 조건과 정당성 증명
하지만 위와 같은 방식으로는 몇 번이나 어떤 순서로 완화를 해야할지가 명확하지 않습니다. 
또한 , upper가 실제 최단 거리와 같아진 다는 것을 어떻게 알 수 있을까요? 그리고 어떤 정점을 택하더라도 upper[u] = dist[u]가 되는 것이 확실할까요?

모든 간선에대해서 완화를 시도하는 작업을 x번 반복하면 x개 이하의 간선을 사용하는 최단 경로들을 전부 찾을 수 있습니다.

따라서 모든 간선이 전부 완화가 실패할 때까지 반복하면 모든 최단 경로를 찾을 수 있습니다.
그렇다면 몇 번을 반복해야 최단 경로를 구할 수 있을지 미리 알 수 있는 방법은 없을까요??
음수 사이클이 없는 그래프에서 최단 경로가 한 정점을 2번 지나는 일이 없다는 특징을 이용하면, 최단 경로가 포함하는 간선의 상한선을 쉽게 알 수 있습니다.

최단 경로는 최대 |V|개의 정점을 갖기 때문에 최대 |V|-1개의 간선을 가질 수 있습니다.
따라서 모든 간선에 대한 완화 과정은 전체 |V|-1번이면 충분합니다.

그렇다면 음수 간선이 존재하는 경우에는 최단 거리를 어떻게 구할 수 있을까요?

### 벨만-포드의 음수 사이클의 판정
그래프에 음수 사이클이 존재할 경우 벨만-포드 알고리즘도 의미없는 값을 반환하게됩니다. 하지만 간단한 변형을 통해서 벨만-포드 알고리즘이 음수 사이클의 존재 여부를 판정하게 만들 수 있습니다.
벨만-포드 알고리즘은 그래프가 음수 사이클이 존재하면 의미없는 값을 반환하는 것이 아니라 음수 사이클이 존재한다는 오류를 반환하게 합니다.

음수 사이클의 존재 여부를 판정하려면 |V|-1번 모든 간선에 대한 완화를 시도하는 대신 1번 더 해서 |V|번 완화를 시도하면 됩니다. 그래프에 음수 사이클이 없다면 |V|-1번만 반복해도 모든 최단 거리를 찾을 수 있기 때문에, 마지막 반복의 완화는 전부 실패할 것이기 때문입니다. 반면, 음수 사이클이 있는 경우에는 |V|번째 반복에도 항상 완화가 한 번은 성공합니다.

### 구현
```java
int V;//그래프의 정점의 개수
ArrayList<Edge> adj = new ArrayList<Edge>();

int[] d = new int[V+1];
void bellmanford(int start){
	for(int i = 1;i<=n;i++){
		d[i] = Integer.MAX_VALUE;
	d[start] = 0;
	for(int i = 1;i<=n-1;i++){
		for(int j = 0;j<adj.size();j++){
			Edge temp = adj.get(j);
			if(d[temp.end] > d[temp.start] + temp.weight){
				d[temp.end] = d[temp.start] + temp.weight;
			}
		}
	}
}
class Edge{
    int start;
    int end;
    int weight;

    public Edge(int start, int end, int weight) {
        this.start = start;
        this.end = end;
        this.weight = weight;
    }
}
```
### 실제 경로 계산하기
벨만-포드 알고리즘을 수행하는 과정에서 각 정점을 마지막으로 완화시킨 간선들을 모으면 스패닝 트리를 얻을 수 있습니다. 각 정점을 마지막으로 완화시킨 간선들은 항상 최단 경로 위에 있기 때문에, 각 정점에서부터 스패닝 트리의 루트인 시작점까지 거슬러 올라가는 경로는 항상 시작점에서 해당 경로까지의 최단경로가 됩니다.
이는 너비 우선 탐색이나 다익스트라 알고리즘과 비슷한 방식으로 실제 정점의 목록을 계산할 수 있습니다.

## 플로이드의 모든 쌍 최단 거리 알고리즘
다익스트라 알고리즘과 벨만-포드 알고리즘은 시작점을 기준으로 다른 정점들까지의 최단 경로를 구하는 알고리즘입니다.
하지만 문제에 따라서는 한 개의 시작점이 아닌 모든 정점 쌍에 대해서 둘 사이의 최단 거리를 구해야 할 때도 있습니다. 
이런 문제를 다익스트라나 벨만-포드 알고리즘을 이용해서 그래프의 존재하는 모든 쌍의 최단 거리를 구하면 시간 복잡도는 다음과 같습니다.
- 다익스트라
	- Linear Array 를 사용한 경우 0(V^3+VE) = 0(V^3)
	- 우선 순위 큐(min-heap)을 사용한 경우 O((V^2)*logV+VE)
- 벨만-포드
	- O(V^2E) = O(V^4) 
이보다 조금 더 빠르고 간단한 방법으로 모든 쌍 간의 최단 거리를 구하는 방법이 플로이드의 모든 쌍 최단 거리 알고리즘입니다.

플로이드 알고리즘은 그래프의 모든 정점 쌍의 최단 거리를 저장하는 2차원 배열 dist[ ][ ]를 계산하는 방식으로 동작합니다. 이 때, dist[u][v]는 u에서 v로 가는 최단 거리를 의미합니다.

플로이드 알고리즘은 경로의 경유점이라는 개념을 이용해서 동작합니다.
> 정점의 경유점
> 두 정점 u,v를 잇는 어떤 경로가 있고 그 경로는 시작점u와 끝점 v를 항상 지난다고 가정하겠습니다.  
> 이 경로는 다른 정점들을 지나쳐 갈 수 있습니다. 그 이유는 u와 v를 직접 연결하는 간선이 없거나, 다른 정점을 경유해서 가는 경로가 전체 경로가 더 짧을 수 있기 때문입니다. 이 때 경로가 거쳐가는 정점들을 경유점이라고 합니다.
정점 집합 S에 포함된 정점만을 경유점으로 사용해서 u에서 v로 가는 최단 경로의 길이를 Ds(u,v)라고 하겠습니다. 

S에 포함된 정점만을 경유점으로 사용해 u에서 v로 가는 최단 경로를 알고있다고 가정하겠습니다. S 중에 정점을 하나 골라서 x라고 하면, 최단 경로는 x를 경유할 수도 있고 경유하지 않을수도 있습니다.

1. 경로가 x를 경유하지 않는다 : 이 경로는 S- {x} 에 포함된 정점들만을 경유점으로 사용합니다.
2. 경로가 x를 경유한다 : 이 경로는 u에서 x로 가는 구간과 x에서 v로 가는 구간으로 나눌 수 있습니다. 이 2개의 부분 경로들은 각각 u와 x, x와 v를 잇는 최단 경로들이어야 합니다. 
당연하게도 두 개의 부분 경로들은 x를 경유하지않으며, 따라서 S-{x}에 포함된 정점들만을 경유점으로 사용합니다.

S를 경유점으로 사용해 u에서 v로 가는 최단 경로는 위 2가지 중 더 짧은 경로가 될 것입니다.
Ds(u,v)를 다음과 같이 재귀적으로 정의할 수 있습니다.

![KakaoTalk_Photo_2021-08-28-22-51-13](https://user-images.githubusercontent.com/16794320/131220123-c9c0150a-a7be-40b9-824d-db66a496d4b8.jpeg)
위의 점화식을 살짝만 수정하면 모든 쌍에대한 최단 거리 문제를 동적 계획법으로 해결할 수 있습니다.

표기법을 살짝 고쳐서 Ck = D_s_k라 하면 다음과 같이 표현할 수 있습니다.
![KakaoTalk_Photo_2021-08-28-23-13-25](https://user-images.githubusercontent.com/16794320/131220778-7b310a52-8fd4-48df-98fc-2db6464dbddb.jpeg)
이 점화식은 C_k의 모든 값은 C_(k-1)에만 의존하기 때문에 동적 계획법을 이용할 수 있습니다.

### 구현
구체적인 구현에 앞서 플로이드 알고리즘의 프로토타입은 다음과 같습니다.
d[k,i,j] = set{1,2,...,k} 에 포함되는 i에서 j 로 가는 최단 경로
k가 0인 경우에는 중간 경로 없는 vertex i에서 vertex j로 바로 가는 경로이기 때문에 d[0,i,j] = w[i,j] 입니다.
```java
for(int k = 0;k<n;k++){
	for(int i = 0;i<n;i++){
		for(int j = 0;j<n;j++){
			if(k == 0) d[k][i][j] = w[i,j];
			else d[k][i][j] = min(d[k-1][i][j],d[k-1][i][k] + d[k-1][k][j]);
		}
	}
}
```
위의 코드에서 볼 수 있듯 플로이드 알고리즘의 시간복잡도는 3중 for문을 돌기 때문에 O(|V|^3)입니다. 공간복잡도 역시 3차원 배열을 사용하기 때문에 (|V|^3) 입니다.
여기서 공간 복잡도를 줄일 수 있는 방법이 있습니다.
k번째 case를 계산할 때 k-1번째의 연산으로 부터 저장된 정보가 overwrite 될 수 있습니다. 그 이유는 출발점이나 도착점이 k번 정점일 때 사용 가능한 경유점의 목록에 k가 추가되는 것은 아무 의미가 없기 때문에, 이를 구분하지 않고 써도 되기 때문입니다.
예를 들면, **지하철 역에 들러 학교로 가는 최단 경로**와 **지하철역과 학교를 들러 학교로 가는 최단 경로**는 똑같기 때문입니다.
이런 이유로 우리는 더이상 3차원 배열을 사용해서 k번째 연산과 k-1번째 연산을 구분할 필요없이 한 개의 2차원 배열을 이용해서 코드를 짤 수 있습니다.
```java
for(int k = 0;k<n;k++){
	for(int i = 0;i<n;i++){
		for(int j = 0;j<n;j++){
			d[k][i][j] = min(d[k-1][i][j],d[k-1][i][k] + d[k-1][k][j]);
		}
	}
}
```
2차원 배열을 사용하면 시간 복잡도는 그대로지만 공간 복잡도는 O(|V|^3)에서 O(|V|^2)로 줄일 수 있습니다.

### 문제 추천
[다익스트라](https://www.acmicpc.net/problem/19701)
[플로이드](https://www.acmicpc.net/problem/11404)
[벨만-포드](https://www.acmicpc.net/problem/13317)

# 최소 스패닝 트리
최소 스패닝 트리(MST)는 그래프에서 만날 수 있는 최소 비용 문제 중 모든 정점을 연결하는 간선들의 가중치의 합이 최소가 되는 트리를 의미합니다.

그렇다면 스패닝 트리는 무엇일까요??
## 스패닝 트리
n개의 정점으로 이루어진 무향 그래프에서 n개의 정점과 n-1개의 간선으로 이루어진 트리를 신장트리라 합니다. 다른 말로, 원래 그래프의 정점 전부와 간선의 부분 집합으로 구성된 부분 그래프 입니다. 이 때, 스패닝 트리에 포함된 간선들은 정점들을 트리 형태로 전부 연결해야 합니다.
이런 특징들로부터 우리는 스패닝 트리가 유일하지 않고 여러개가 존재합니다. 
가중치 그래프의 여러 개의 스패닝 트리 중 가중치의 합이 가장 작은 트리를 찾는 문제입니다.
## 크루스칼의 최소 스패닝 트리 알고리즘
크루스칼의 알고리즘은 상호 배타적 집합 자료 구조를 사용하는 좋은 예입니다. 
크루스칼 알고리즘을 접근하기 전에 다음 질문의 답을 생각해보겠습니다.
> 가중치가 가장 작은 간선과 가중치가 가장 큰 간선 중 어느 쪽이 최소 스패닝 트리에 포함될 가능성이 높을까?

대부분 가중치가 가장 작은 간선일 것입니다. 크루스칼의 알고리즘은 여기서 출발합니다.

그래프의 모든 간선을 가중치의 오름차순으로 정렬합니다. 그 후, 스패닝트리에 하나씩 추가합니다. 이때 주의할 점은, 간선들이 사이클을 이루지 않게 해야하는 것입니다. 그렇기 때문에, 가중치가 작다고 무조건 간선을 트리에 더하는 것이 아닌, 결과적으로 사이클이 생기는 간선을 제외한 간선 중 가중치가 가장 작은 간선들을 트리에 추가합니다.

이처럼 크루스칼 알고리즘은 모든 간선을 한 번씩 감사한 뒤 종료합니다.
다음은 크루스칼 알고리즘이 최소 스패닝 트리를 만드는 과정을 표현한 그림입니다.

![](https://images.velog.io/images/hongcheol/post/a6cdfd6c-3af9-4afe-aa18-41ca586b0533/KakaoTalk_Photo_2021-08-24-17-48-50.gif)

### 크루스칼 알고리즘의 구현
```java
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Arrays;
import java.util.Comparator;
import java.util.StringTokenizer;

public class KuruskalTest {

    static class Edge implements Comparable<Edge> {
        int start, end, weight;
        public Edge(int start,int end, int weight){
            this.start = start;
            this.end = end;
            this.weight = weight;
        }
        @Override
        public int compareTo(Edge o){
            return Integer.compare(this.weight,o.weight);
        }
    }
    static int V;
    static int E;
    static BufferedReader br = new BufferedReader(new InputStreamReader(System.in));
    static StringTokenizer st;
    static Edge[] edgeList;
    public static void main(String[] args) throws IOException {
        st = new StringTokenizer(br.readLine()," ");
        V = Integer.parseInt(st.nextToken());
        E = Integer.parseInt(st.nextToken());

        edgeList = new Edge[E];
        for(int i = 0;i<E;i++){
            st = new StringTokenizer(br.readLine()," ");
            int start = Integer.parseInt(st.nextToken());
            int end = Integer.parseInt(st.nextToken());
            int weight = Integer.parseInt(st.nextToken());
            edgeList[i] = new Edge(start,end,weight);
        }
        Arrays.sort(edgeList);//오름차순
        make();// 모든 정점을 각각 집합으로 만들고 출발한다.
        //간선 하나씩 시도하며 트리를 만든다.
        int cnt = 0,result = 0;
        for(Edge edge : edgeList){
            if(merge(edge.start,edge.end)){
                result += edge.weight;
                if(++cnt == V-1) break;// 신장트리 완성.
            }
        }
        System.out.println(result);

    }
    static int[] parents;
    static void make(){
        parents = new int[V];
        for(int i = 0;i<V;i++){
            parents[i] = i;
        }
    }
    //u가 속한 트리의 루트 번호를 반환한다.
    static int find(int u){
        if(u == parents[u]) return u;
        //return find(parent[u]); --- 기울어진 트리의 경우 비효율적
        //최적화(Path Compression)
        return parents[u] = find(parents[u]);
    }
    //u가 속한 트리와 v가 속한 트리를 합친다..
    static boolean merge(int u, int v){
        u = find(u);
        v = find(v);
        //u와 v가 이미 같은 트리에 속하는 경우는 걸러낸다.
        if(u ==v) return false;
        parents[u] = v;
        return true;
    }
}

```
### 정당성 증명
1. 크루스칼 알고리즘이 선택하는 간선 중 그래프의 최소 스패닝 트리 T에 포함되지않는 간선이 있다고 가정
2. 이 중 첫번째로 선택되는 간선을 (u,v)라 하자. T는 이 간선을 포함하지않기 때문에, u와 v는 T에서 다른 경로로 연결되어 있을 것이다.
3. 이 경로를 이루는 간선 중 하나는 반드시 (u,v)와 가중치가 크거나 같아야한다.(그 이유는 모두 (u,v)보다 가중치가 작다면 크루스칼 알고리즘이 이미 이 간선들을 모두 선택해서 u와 v를 연결했을 것이기 때문에 (u,v)가 선택됐을리 없다.
4. 따라서 이 경로 상에서 (u,v) 이상의 가중치를 갖는 간선을 하나 골라서 T에서 지워버리고 (u,v)를 추가해도 스패닝 트리는 유지되면서 가중치의 총합은 줄거나 같을 것입니다.
5. 하지만 T가 이미 최소 스패닝 트리라고 가정했기 때문에, (u,v)를 포함하면서 최소 스패닝 트리가 되어야합니다.
6. 따라서 (u,v)를 선택한다고 하더라도 남은 간선들을 잘 선택하면 항상 최소 스패닝 트리를 얻을 수 있습니다.
1~6의 성질은 마지막 간선을 추가해 스패닝 트리가 완성될 때까지 성립하기 때문에, 마지막에 얻은 트리는 항상 최소 스패닝 트리가 됩니다.

### 시간복잡도
DisJointSet에대한 연산은 실질적으로 상수기간이기 때문에,실제 트리를 만드는 for문의 시간복잡도 O(|E|)입니다. 따라서 크루스칼 알고리즘의 전체 시간복잡도는 간선 목록의 정렬에 걸리는 시간 O(|E|log|E|)가 됩니다. 간선 목록의 정렬하는 시간이 알고리즘 전체 시간 중에 지배적으로 크기 때문에, 간선의 수가 많아지면 크루스칼 알고리즘은 효율이 떨어집니다.

## 프림의 최소 스패닝 트리 알고리즘
프림의 알고리즘은 다익스트라 알고리즘과 거의 같은 형태를 띠고 있습니다.
크루스칼 알고리즘이 여기저기서 산발적으로 만들어진 트리의 조각들을 합쳐서 스패닝 트리를 만든다면, 프림 알고리즘은 하나의 시작점으로 구성된 트리에 간선을 하나씩 추가하는 방식으로 진행됩니다. 그렇기 때문에, 항상 선택된 간선들은 중간 과정에서도 연결된 트리를 만듭니다.

프림 알고리즘은 선택할 수 있는 간선들 중 가중치가 가장 작은 간선을 선택하는 과정을 반복합니다. 
아래는 프림 알고리즘을 이용해서 최소 스패닝 트리를 만드는 과정을 표현한 그림입니다.
![](https://images.velog.io/images/hongcheol/post/bd40fe46-3300-4d2d-be50-39583189be28/KakaoTalk_Photo_2021-08-24-17-30-01%20001.gif)
파란색으로 표현된 선은 이번 단계에서 고려할 간선이고, 그 중 선택된 간선을 하늘색으로 표현했습니다. 초록색 선은 이미 선택된 간선들을 의미합니다.

### 프림 알고리즘의 구현
```java
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.StringTokenizer;

public class PrimTest {
    public static void main(String[] args) throws IOException {
        BufferedReader br = new BufferedReader(new InputStreamReader(System.in));
        int N = Integer.parseInt(br.readLine());
        int[][] adjMatrix = new int[N][N];
        boolean[] visited = new boolean[N];
        int[] minEdge = new int[N];

        StringTokenizer st = null;
        for(int i = 0;i<N;i++){
            st = new StringTokenizer(br.readLine()," ");
            for(int j = 0;j<N;j++){
                adjMatrix[i][j] = Integer.parseInt(st.nextToken());
            }
            minEdge[i] = Integer.MAX_VALUE;
        }

        int result = 0;//최소 신장 트리 비용
        minEdge[0] = 0;//임의의 시작점 0의 간선 비용을 0으로 세팅 index는 아무거나 상관없다.
        for(int i = 0;i<N;i++){
            // 1. 신장 트리에 포함되지않은 정점 중 최소간선비용의 정점 찾기
            int min = Integer.MAX_VALUE;
            int minVertex = -1;//최소간선비용의 정점번호
            for(int j = 0;j<N;j++){
                if(!visited[j] && min>minEdge[j]){
                    min = minEdge[j];
                    minVertex = j;
                }
            }
            visited[minVertex] = true;//신장트리에 포함시킴.
            result += min;//간선비용 누적.

            //2. 선택된 정점 기준으로 신장트리에 연결되지않은 타 정점과의 간선 비용 최소로 업데이트
            for (int j = 0; j < N; j++) {
                //인접 안해있으면 인풋이 0이므로 걔가 최소가 되어버림.
                if(!visited[j] && adjMatrix[minVertex][j]!=0 && minEdge[j] > adjMatrix[minVertex][j]){
                    minEdge[j] = adjMatrix[minVertex][j];
                }
            }
        }
        System.out.println(result);
    }
}
```
다익스트라 알고리즘의 구현과 비슷한 코드입니다.
각 정점에대해서 지금까지 알려진 최단 거리를 저장하는 것이 아닌, 마지막 간선의 가중치를 저장하는 방식으로 구현했습니다.
우선 순위 큐를 이용해서 최소 간선 비용의 정점을 찾으면 코드를 최적화 할 수 있습니다. 이렇게 구현을 하면, 우선순위 큐는 minEdge[ ]가 증가하는 순서로 정렬해서 담고있게 됩니다.

### 정당성 증명
크루스칼 알고리즘의 증명과 똑같이 증명할 수 있습니다.

### 크루스칼 vs 프림
크루스칼이 간선 위주였다면 프림은 정점을 위주로 풀어나가는 알고리즘입니다.
둘은 이미 만들어진 트리에 인접한 간선을 고려하는지의 여부를 제외하면 완전히 똑같은 알고리즘입니다.

### 문제 추천
[MST](https://www.acmicpc.net/problem/1197)
